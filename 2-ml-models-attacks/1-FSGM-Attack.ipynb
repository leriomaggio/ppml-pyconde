{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext notexbook\n",
    "%texify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast Gradient Sign Attack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook showcases how to carry out a **Fast Gradient Sign Attack** (`FGSA`) to a pretrained model. \n",
    "\n",
    "**Note** This notebook has been adapted from the [FSGM Tutorial](https://pytorch.org/tutorials/beginner/fgsm_tutorial.html) by _Nathan Inkawhich_ `@inkawhich` available on the official [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(_from the original notebook_)\n",
    "> This tutorial will raise your awareness to the security vulnerabilities \n",
    "> of ML models, and will give insight into the hot topic of adversarial machine learning. \n",
    "> \n",
    "> You may be surprised to find that adding **imperceptible perturbations** to an image *can* cause \n",
    "> drastically different model performance.\n",
    "> `[...]`\n",
    ">\n",
    "> Specifically we will use one of the first and most popular attack methods, the _Fast Gradient Sign Attack_\n",
    "> (`FGSM`), to fool an `MNIST` classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threats to Models\n",
    "\n",
    "There are several kinds of assumptions of the attacker’s knowledge, two of which are: **white-box** and **black-box**. \n",
    "\n",
    "- A *white-box* attack assumes the attacker has full knowledge and access to the model, including\n",
    "architecture, inputs, outputs, and weights. \n",
    "- A *black-box* attack assumes the attacker only has access to the inputs and outputs of the model, and knows nothing about the underlying architecture or weights. \n",
    "\n",
    "There are also several types of goals, including **misclassification** and\n",
    "**source/target misclassification**. \n",
    "\n",
    "A goal of *misclassification* means the adversary only wants the output classification to be wrong but does\n",
    "not care what the new classification is. \n",
    "\n",
    "A *source/target misclassification* means the adversary wants to alter an image that is originally of a specific source class so that it is classified as a specific target class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast Gradient Sign Attack\n",
    "\n",
    "*Fast Gradient Sign Attack (FGSM)* and is described by _Goodfellow et. al._ in \n",
    "[Explaining and Harnessing Adversarial Examples](https://arxiv.org/abs/1412.6572). \n",
    "\n",
    "The attack is remarkably powerful, and yet intuitive. \n",
    "\n",
    "It is designed to attack neural networks by leveraging the way they learn: **gradients**. \n",
    "\n",
    "The idea is simple: \n",
    "\n",
    "> rather than working to minimize the loss by adjusting the weights based on the backpropagated gradients,\n",
    "> the attack **adjusts** the input data to maximize the loss based on the same backpropagated gradients. \n",
    "\n",
    "In other words, the attack uses the gradient of the loss w.r.t the input data, then adjusts the input data to maximize the loss.\n",
    "\n",
    "_(from the original paper)_\n",
    "\n",
    "![fgsm panda attack](https://pytorch.org/tutorials/_images/fgsm_panda_image.png)\n",
    "\n",
    "**TLDR;** Just perturbe the input data with some small change that would work in an **adversary** fashion (wrt. the optimisation process) that follows the **direction of the gradient** (i.e. $sign(\\nabla_{x} J(\\mathbf{\\theta}, \\mathbf{x}, y))$ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NOTE: This is a hack to get around \"User-agent\" limitations when downloading MNIST datasets\n",
    "#       see, https://github.com/pytorch/vision/issues/3497 for more information\n",
    "from six.moves import urllib\n",
    "\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders = [(\"User-agent\", \"Mozilla/5.0\")]\n",
    "urllib.request.install_opener(opener)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os \n",
    "\n",
    "DATA_FOLDER = Path(os.path.join(os.path.abspath(os.path.curdir), \"..\")) / \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility Settings\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "SEED = 123456\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `LeNet` Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeNet Model definition\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise pre-trained model (and move it to available device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    dev_name = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    dev_name = \"mps\"\n",
    "else:\n",
    "    dev_name = \"cpu\"\n",
    "\n",
    "device = torch.device(dev_name)\n",
    "print(f\"You will be using the {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this when running on Anaconda Notebooks\n",
    "# !wget !wget https://raw.githubusercontent.com/leriomaggio/ppml-tutorial/main/3-ml-models-attacks/lenet_mnist_model.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_MODEL_WEIGHTS = \"lenet_mnist_model.pth\"\n",
    "\n",
    "# Initialize the network\n",
    "model = Net().to(device)\n",
    "\n",
    "# Load the pretrained model\n",
    "model.load_state_dict(torch.load(PRETRAINED_MODEL_WEIGHTS, map_location=device))\n",
    "\n",
    "# Set the model in evaluation mode. In this case this is for the Dropout layers\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST Test dataset and dataloader declaration\n",
    "mnist_test = datasets.MNIST(root=DATA_FOLDER, train=False, download=True, transform=transforms.ToTensor())\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before the Attack\n",
    "\n",
    "Before carrying out the attack, let's see how well the model classify the digits in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "y_preds, y_true = list(), list()\n",
    "with torch.no_grad():  # extra, as model is eval mode anyway\n",
    "    for (image, target) in tqdm(test_loader):\n",
    "        image, target = image.to(device), target.to(device)\n",
    "        out = model(image)\n",
    "        _, preds = torch.max(out, 1)\n",
    "        y_preds.append(preds.detach().cpu().numpy())\n",
    "        y_true.append(target.detach().cpu().numpy())\n",
    "    y_preds = np.hstack(y_preds)\n",
    "    y_true = np.hstack(y_true)\n",
    "    \n",
    "    print(f\"Pre-Trained Model ACC: {accuracy_score(y_true, y_preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `FSGM` Attack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can define the function that creates the adversarial examples by\n",
    "perturbing the original inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FGSM attack code\n",
    "def fgsm_attack(image: torch.Tensor, epsilon: float, data_gradient: torch.Tensor) -> torch.Tensor:\n",
    "    # Collect the element-wise sign of the data gradient\n",
    "    sign_data_grad = data_gradient.sign()\n",
    "    # Create the perturbed image by adjusting each pixel of the input image\n",
    "    perturbed_image = image + (epsilon * sign_data_grad)\n",
    "    # Adding clipping to maintain [0,1] range\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)  # normalise in [0, 1] to make it an actual image\n",
    "    # Return the perturbed image\n",
    "    return perturbed_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least: the **test function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, loader, epsilon):\n",
    "    # from https://github.com/pytorch/tutorials/blob/master/beginner_source/fgsm_tutorial.py\n",
    "    \n",
    "    # Accuracy counter\n",
    "    correct = 0\n",
    "    adv_examples = []\n",
    "\n",
    "    # Loop over all examples in test set\n",
    "    for data, target in tqdm(test_loader, desc=f\"Running Attack on Batches with ε={ε}\"):\n",
    "\n",
    "        # Send the data and label to the device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Set requires_grad attribute of tensor. Important for Attack\n",
    "        data.requires_grad = True\n",
    "\n",
    "        # Forward pass the data through the model\n",
    "        output = model(data)\n",
    "        init_pred = output.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "\n",
    "        # If the initial prediction is wrong, don't bother attacking, just move on\n",
    "        if init_pred.item() != target.item():\n",
    "            continue\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        # Zero all existing gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Calculate gradients of model in backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Collect datagrad\n",
    "        data_grad = data.grad.data\n",
    "\n",
    "        # Call FGSM Attack\n",
    "        perturbed_data = fgsm_attack(data, epsilon, data_grad)\n",
    "\n",
    "        # Re-classify the perturbed image\n",
    "        output = model(perturbed_data)\n",
    "\n",
    "        # Check for success\n",
    "        final_pred = output.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "        if final_pred.item() == target.item():\n",
    "            correct += 1\n",
    "        else:\n",
    "            # Save some adv examples for visualization later\n",
    "            if len(adv_examples) < 5:\n",
    "                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                adv_examples.append((init_pred.item(), final_pred.item(), adv_ex))\n",
    "\n",
    "    # Calculate final accuracy for this epsilon\n",
    "    final_acc = correct / float(len(test_loader))\n",
    "    print(\n",
    "        \"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(\n",
    "            epsilon, correct, len(test_loader), final_acc\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Return the accuracy and an adversarial example\n",
    "    return final_acc, adv_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ε = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, adv_examples = test(model, device, test_loader, ε)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how the perturbed images look like: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, len(adv_examples), figsize=(8, 10))\n",
    "plt.xticks([], [])\n",
    "plt.yticks([], [])\n",
    "for j, (orig_pred, adv_pred, adv_example) in enumerate(adv_examples):\n",
    "    if j == 0:\n",
    "        axes[j].set_ylabel(f\"ε: {ε}\", fontsize=14)\n",
    "    axes[j].set_title(\"{} -> {}\".format(orig_pred, adv_pred))\n",
    "    axes[j].imshow(adv_example, cmap=\"gray\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "Now the question is: how much degradation in performance we have as soon as we keep incrementing the value of ε?\n",
    "\n",
    "What we should expect: \n",
    "- the bigger ε, the worse the accuracy\n",
    "- the bigger ε, the more \"discoverable\" the perturbation becomes\n",
    "    - so that it's evident that an attack has been launched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILONS = [0.05, .06, .1, .15, .2, .25, .3]\n",
    "\n",
    "accuracies = [acc]\n",
    "adv_examples_map = {0.05: adv_examples}\n",
    "\n",
    "# Run test for each epsilon\n",
    "for ε in EPSILONS[1:]:\n",
    "    acc, adv_examples = test(model, device, test_loader, ε)\n",
    "    accuracies.append(acc)\n",
    "    adv_examples_map[ε] = adv_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "1. Let's print the accuracy values for each corresponding ε value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "# your code here: plot Accuracies vs EPSILONS\n",
    "plt.plot(EPSILONS, accuracies)\n",
    "plt.yticks(np.arange(0, 1.1, step=0.1))\n",
    "plt.xticks(np.arange(0, 0.35, step=0.05))\n",
    "plt.title(\"Accuracy vs Epsilon\")\n",
    "plt.xlabel(\"Epsilon\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Visualise Generated Adversarial Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot several examples of adversarial samples at each epsilon\n",
    "\n",
    "for ε in EPSILONS:\n",
    "    fig, axes = plt.subplots(1, len(adv_examples_map[ε]), figsize=(8, 10))\n",
    "    plt.xticks([], [])\n",
    "    plt.yticks([], [])\n",
    "    for j, (orig_pred, adv_pred, adv_example) in enumerate(adv_examples_map[ε]):\n",
    "        if j == 0:\n",
    "            axes[j].set_ylabel(f\"ε: {ε}\", fontsize=14)\n",
    "        axes[j].set_title(\"{} -> {}\".format(orig_pred, adv_pred))\n",
    "        axes[j].imshow(adv_example, cmap=\"gray\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
