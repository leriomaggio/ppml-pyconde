{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing Opacus\n",
    "\n",
    "![](https://opacus.ai/img/opacus_logo_vertical.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Opacus](https://opacus.ai/) is the framework in the PyTorch ecosystem that brings **Differential Privacy** to (PyTorch) Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "(from [Introducing Opacus](https://ai.facebook.com/blog/introducing-opacus-a-high-speed-library-for-training-pytorch-models-with-differential-privacy/))\n",
    "\n",
    " \n",
    "> `Opacus` (_is_) a new high-speed library for training PyTorch models with differential privacy (DP) that’s more scalable than existing state-of-the-art methods. \n",
    ">\n",
    "> Differential privacy is a mathematically rigorous framework for quantifying the anonymization of sensitive data. \n",
    ">\n",
    "> It’s often used in analytics, with growing interest in the machine learning (ML) community. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differential Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like $k$-Anonymity, *differential privacy*[3](#fn3) is a **formal notion of privacy** \n",
    "(i.e. it's possible to prove that a data release has the property). \n",
    "\n",
    "Unlike $k$-Anonymity, however, **differential privacy** is a property of *algorithms*, and not a property of *data*. \n",
    "\n",
    "That is, we can prove a *dataset* satisfies differential privacy by proving that an *algorithm* satisfies differential privacy.\n",
    "\n",
    "> **Definition**:\n",
    ">\n",
    "> A function which satisfies differential privacy is often called a *mechanism*. \n",
    "> We say that a *mechanism* $F$ satisfies differential privacy if for all *neighboring datasets* $x$ and $x'$, \n",
    "> and all possible outputs $S$,\n",
    ">\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\mathsf{Pr}[F(x) = S]}{\\mathsf{Pr}[F(x') = S]} \\leq e^\\epsilon\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Neighbours Datasets**:\n",
    "\n",
    "Two datasets are considered **neighbours** if they differ in the data by **one single individual**.\n",
    "\n",
    "**2. $F$ Randomised Function**:\n",
    "\n",
    "Note that $F$ is typically a *randomised* function, so that the probability distribution describing its outputs is not just a point distribution.\n",
    "\n",
    "The important implication of this definition is that $F$'s output will be pretty much the same, *with or without* the data of any specific individual.\n",
    "\n",
    "In other words, the randomness built into $F$ should be \"enough\" so that an observed output from $F$ will not reveal which of $x$ or $x'$ was the input.\n",
    "\n",
    "Imagine that my data is present in $x$ but not in $x'$.\n",
    "\n",
    "**3. The Privacy Budget: $\\epsilon$**:\n",
    "\n",
    "If an adversary can't determine which of $x$ or $x'$ was the input to $F$, then the adversary can't tell whether or not my data was *present* in the input - let alone the contents of that data.\n",
    "\n",
    "The $\\epsilon$ parameter in the definition is called the *privacy parameter* or the *privacy budget*.\n",
    "\n",
    "$\\epsilon$ provides a knob to tune the **amount of privacy** the definition provides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small values of $\\epsilon$ require $F$ to provide *very* similar outputs when given similar inputs, and therefore provide **higher levels** of privacy.\n",
    "\n",
    "Large values of $\\epsilon$ allow less similarity in the outputs, and therefore provide **less privacy**.\n",
    "\n",
    "\n",
    "- Small values $\\epsilon \\rightarrow$ High Privacy\n",
    "- Large values $\\epsilon \\rightarrow$ Less Privacy\n",
    "\n",
    "How should we set $\\epsilon$ to prevent bad outcomes in practice? **Nobody knows** (i.e. Open Research Question). \n",
    "\n",
    "The general consensus is that $\\epsilon$ should be around `1` or smaller, and values of $\\epsilon$ above `10` probably don't do much to protect privacy - but this rule of thumb could turn out to be very conservative. \n",
    "\n",
    "<span id=\"fn3\">**[3]**: Dwork, C; _Differential Privacy_ in Proceedings of the 33rd International Conference on Automata, Languages and Programming - Volume Part II, 2006 [link](https://doi.org/10.1007/11787006_1)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Properties of Differential Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will mention three important properties of **differentially private mechanisms** that arise from the definition\n",
    " of differential privacy.\n",
    "\n",
    "These properties are mentioned as they will be used / referenced when we will start generalising DP applications \n",
    "to Machine Learning algorithms.\n",
    "\n",
    "\n",
    "These three properties are:\n",
    "\n",
    "1. Sequential composition\n",
    "2. Parallel composition\n",
    "3. Post processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Composition\n",
    "\n",
    "The first major property of differential privacy is *sequential composition*, which **bounds** the total \n",
    "privacy cost of releasing multiple results of differentially private mechanisms **on the same input data**. \n",
    "\n",
    "Formally, the sequential composition theorem for differential privacy says that:\n",
    "\n",
    "- If $F_1(x)$ satisfies $\\epsilon_1$-differential privacy\n",
    "- And $F_2(x)$ satisfies $\\epsilon_2$-differential privacy\n",
    "- Then the mechanism $G(x) = (F_1(x), F_2(x))$ which releases both results satisfies $\\epsilon_1+\\epsilon_2$-differential privacy\n",
    "\n",
    "\n",
    "Sequential composition is a vital property of differential privacy because it enables the design of algorithms that consult the data more than once. \n",
    "\n",
    "Sequential composition is also important when multiple separate analyses are performed on a single dataset, since it allows individuals to bound the *total* privacy cost they incur by participating in all of these analyses.\n",
    "\n",
    "The bound on privacy cost given by sequential composition is an *upper* bound - the actual privacy cost of two particular differentially private releases may be smaller than this, but never larger.\n",
    "\n",
    "The principle that the $\\epsilon$-s \"add up\" makes sense if we examine the distribution of outputs from a mechanism which averages two differentially private results together.\n",
    "\n",
    "However, please bear in mind that Sequential composition does not provide an **exact** upper bound: the exact total privacy cost can be indeed lower than the upper bound!."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Composition\n",
    "\n",
    "The second important property of differential privacy is called *parallel composition*. \n",
    "\n",
    "Parallel composition can be seen as an alternative to sequential composition - a second way to calculate a bound on the total privacy cost of multiple data releases. \n",
    "\n",
    "Parallel composition is based on the idea of **splitting** your dataset into disjoint chunks and running a \n",
    "differentially private mechanism on each chunk separately. \n",
    "\n",
    "Since the chunks are **disjoint**, each individual's data appears in *exactly* one chunk - so even if there are $k$ chunks in total (and therefore $k$ runs of the mechanism), the mechanism runs exactly once on the data of each *individual*. \n",
    "\n",
    "Formally,\n",
    " - If $F(x)$ satisfies $\\epsilon$-differential privacy\n",
    " - And we split a dataset $X$ into $k$ disjoint chunks such that $x_1 \\cup ... \\cup x_k = X$\n",
    " - Then the mechanism which releases all of the results $F(x_1), ..., F(x_k)$ satisfies $\\epsilon$-differential privacy\n",
    "\n",
    "Note that this is a much better bound than sequential composition would give. \n",
    "\n",
    "Since we run $F$ $k$ times, sequential composition would say that this procedure satisfies $k\\epsilon$-differential privacy. \n",
    "\n",
    "Parallel composition allows us to say that the total privacy cost is just $\\epsilon$.\n",
    "\n",
    "The formal definition matches up with our intuition - if each participant in the dataset contributes one row to $X$, then this row will appear in *exactly* one of the chunks $x_1, ..., x_k$. \n",
    "\n",
    "That means $F$ will only \"see\" this participant's data *one time*, meaning a privacy cost of $\\epsilon$ is appropriate for that individual. Since this property holds for all individuals, the privacy cost is $\\epsilon$ for everyone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Post-processing\n",
    "\n",
    "The third property of differential privacy is called *post-processing*. \n",
    "\n",
    "The idea is simple: it's impossible to **reverse the privacy protection** provided by differential privacy by post-processing the data in some way. \n",
    "\n",
    "Formally:\n",
    "\n",
    "- If $F(X)$ satisfies $\\epsilon$-differential privacy\n",
    "- Then for any (deterministic or randomized) function $g$, $g(F(X))$ satisfies $\\epsilon$-differential privacy\n",
    "\n",
    "The post-processing property means that it's always safe to perform arbitrary computations on the output of a differentially private mechanism - there's no danger of reversing the privacy protection the mechanism has provided. \n",
    "\n",
    "In particular, it's fine to perform post-processing that might reduce the noise or improve the signal in the mechanism's output (e.g. replacing negative results with zeros, for queries that shouldn't return negative results). \n",
    "\n",
    "The other implication of the **post-processing** property is that differential privacy provides resistance against privacy attacks based on **auxiliary information**. \n",
    "\n",
    "For example, the function $g$ might contain auxiliary information about elements of the dataset, and attempt to perform a linkage attack using this information. The post-processing property says that such an attack is limited in its effectiveness by the privacy parameter $\\epsilon$, regardless of the auxiliary information contained in $g$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Laplace Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differential privacy is typically used to answer specific queries. Let's consider a query on the census data, *without* differential privacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_URL = \"https://raw.githubusercontent.com/uvm-plaid/programming-dp/master/notebooks/adult_with_pii.csv\"\n",
    "adult = pd.read_csv(DATASET_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** \"How many individuals in the dataset are 40 years old or older?\"\n",
    "\n",
    "This is an example of a **Count Query**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14237"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult[adult['Age'] >= 40].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplace Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to achieve differential privacy for this query is to add **random noise to its answer**. \n",
    "\n",
    "The key challenge is to add enough noise to satisfy the definition of differential privacy, but not so much that the answer becomes too noisy to be useful. \n",
    "\n",
    "To make this process easier, some basic *mechanisms* have been developed in the field of differential privacy, which describe exactly what kind of - and how much - noise to use. \n",
    "\n",
    "One of these is called the *Laplace mechanism*[4](#fn4).\n",
    "\n",
    "> **Definition**\n",
    "> \n",
    ">According to the Laplace mechanism, for a function $f(x)$ which returns a number, the following definition of $F(x)$ satisfies $\\epsilon$-differential privacy:\n",
    ">\n",
    ">\\begin{equation}\n",
    "F(x) = f(x) + \\textsf{Lap}(\\frac{s}{\\epsilon})\n",
    "\\end{equation}\n",
    ">\n",
    ">where $s$ is the *sensitivity* of $f$, and $\\textsf{Lap}(S)$ denotes sampling from the Laplace distribution with center 0 and scale $S$.\n",
    "\n",
    "\n",
    "**Sensitivity**:\n",
    "\n",
    "The *sensitivity* of a function $f$ is the amount $f$'s output changes when its input changes by 1. \n",
    "\n",
    "Sensitivity is a complex topic, and an integral part of designing differentially private algorithms. \n",
    "\n",
    "Let's just point out that *counting queries* always have a sensitivity of `1`: if a query counts the number of rows in the dataset with a particular property, and then we modify exactly one row of the dataset, then the query's output can change by at most `1`.\n",
    "\n",
    "Thus we can achieve differential privacy for our example query by using the `Laplace mechanism` with `sensitivity=1` and an $\\epsilon$ of our choosing.\n",
    "\n",
    "For now, let's pick $\\epsilon = 0.1$. We can sample from the Laplace distribution using Numpy's `random.laplace`.\n",
    "\n",
    "<span id=\"fn4\">**[4]**: Dwork, C.; _Calibrating Noise to Sensitivity in Private Data Analysis_ in Proceedings of the Third Conference on Theory of Cryptography, 2006 [link](https://doi.org/10.1007/11681878_14)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14256.606434525418"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(12345)\n",
    "\n",
    "sensitivity = 1\n",
    "epsilon = 0.1\n",
    "\n",
    "adult[adult['Age'] >= 40].shape[0] + np.random.laplace(loc=0, scale=sensitivity/epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the effect of the noise by running this code multiple times. Each time, the output changes, but most of the time, the answer is close enough to the true answer (14,235) to be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Count Statistic: 14237\n",
      "0) 14232.42\n",
      "1) 14227.00\n",
      "2) 14228.06\n",
      "3) 14238.46\n",
      "4) 14239.12\n",
      "5) 14263.45\n",
      "6) 14240.66\n",
      "7) 14243.89\n",
      "8) 14240.67\n",
      "9) 14243.84\n",
      "10) 14262.59\n",
      "11) 14196.12\n",
      "12) 14221.53\n",
      "13) 14231.85\n",
      "14) 14240.75\n",
      "15) 14246.67\n",
      "16) 14250.64\n",
      "17) 14263.49\n",
      "18) 14242.93\n",
      "19) 14240.35\n",
      "20) 14242.71\n",
      "21) 14236.33\n",
      "22) 14232.71\n",
      "23) 14235.71\n",
      "24) 14243.15\n",
      "25) 14281.25\n",
      "26) 14241.37\n",
      "27) 14245.71\n",
      "28) 14226.27\n",
      "29) 14207.76\n"
     ]
    }
   ],
   "source": [
    "true_count_stat = adult[adult['Age'] >= 40].shape[0]\n",
    "Lap = np.random.laplace(loc=0, scale=sensitivity/epsilon, size=30)\n",
    "print(f\"True Count Statistic: {true_count_stat}\")\n",
    "for i in range(30):\n",
    "    print(f\"{i}) {(true_count_stat + Lap[i]):0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Much Noise is Enough?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we know that the Laplace mechanism adds enough noise to prevent the re-identification of individuals in the dataset? \n",
    "\n",
    "For one thing, we can try to break it!\n",
    "\n",
    "Let's write down a **malicious counting query**, which is specifically designed to determine whether Karrie Trusslove has an income greater than `$50k`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "karries_row = adult[adult['Name'] == 'Karrie Trusslove']\n",
    "karries_row[karries_row['Target'] == '<=50K'].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result definitely violates Karrie's privacy, since it reveals the value of the income column for Karrie's row.\n",
    "\n",
    "Since we know how to ensure differential privacy for counting queries with the Laplace mechanism, we can do so for this query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.181436671261721"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensitivity = 1\n",
    "epsilon = 0.1\n",
    "\n",
    "karries_row = adult[adult['Name'] == 'Karrie Trusslove']\n",
    "karries_row[karries_row['Target'] == '<=50K'].shape[0] + np.random.laplace(loc=0, scale=sensitivity/epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = karries_row[karries_row['Target'] == '<=50K'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.47373848723544"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F = query + np.random.laplace(loc=0, scale=sensitivity/epsilon)\n",
    "\n",
    "np.mean([F for _ in range(100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this the true answer ?\n",
    "\n",
    "There's too much noise to be able to reliably tell.\n",
    "\n",
    "This is how differential privacy is *intended* to work - the approach does not *reject* queries which are determined to be malicious; instead, it adds enough noise that the results of a malicious query will be useless to the adversary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
