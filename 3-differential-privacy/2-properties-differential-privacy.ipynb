{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a9dd8d-ef11-44d7-ac7f-d63171501c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: setup NoTexBook theme\n",
    "%load_ext notexbook\n",
    "%texify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90ec924",
   "metadata": {},
   "source": [
    "**Adapted from**: [Ch4](https://github.com/uvm-plaid/programming-dp/blob/master/notebooks/ch4.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06e8e58-0d12-482b-8085-ba048e5c6e62",
   "metadata": {},
   "source": [
    "# Properties of Differential Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae6c29f-0bf9-483d-be70-72a8a2088a34",
   "metadata": {},
   "source": [
    "In this notebook we will mention three important properties of **differentially private mechanisms** that arise from the definition\n",
    " of differential privacy.\n",
    "\n",
    "These properties are mentioned as they will be used / referenced when we will start generalising DP applications \n",
    "to Machine Learning algorithms.\n",
    "\n",
    "\n",
    "These three properties are:\n",
    "\n",
    "1. Sequential composition\n",
    "2. Parallel composition\n",
    "3. Post processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b6925b-22f4-4841-8217-e6ef69d0ab88",
   "metadata": {},
   "source": [
    "## Sequential Composition\n",
    "\n",
    "The first major property of differential privacy is *sequential composition*, which **bounds** the total \n",
    "privacy cost of releasing multiple results of differentially private mechanisms **on the same input data**. \n",
    "\n",
    "Formally, the sequential composition theorem for differential privacy says that:\n",
    "\n",
    "- If $F_1(x)$ satisfies $\\epsilon_1$-differential privacy\n",
    "- And $F_2(x)$ satisfies $\\epsilon_2$-differential privacy\n",
    "- Then the mechanism $G(x) = (F_1(x), F_2(x))$ which releases both results satisfies $\\epsilon_1+\\epsilon_2$-differential privacy\n",
    "\n",
    "\n",
    "Sequential composition is a vital property of differential privacy because it enables the design of algorithms that consult the data more than once. \n",
    "\n",
    "Sequential composition is also important when multiple separate analyses are performed on a single dataset, since it allows individuals to bound the *total* privacy cost they incur by participating in all of these analyses.\n",
    "\n",
    "The bound on privacy cost given by sequential composition is an *upper* bound - the actual privacy cost of two particular differentially private releases may be smaller than this, but never larger.\n",
    "\n",
    "The principle that the $\\epsilon$-s \"add up\" makes sense if we examine the distribution of outputs from a mechanism which averages two differentially private results together.\n",
    "\n",
    "However, please bear in mind that Sequential composition does not provide an **exact** upper bound: the exact total privacy cost can be indeed lower than the upper bound!."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c48694-ad91-47b6-9467-0fc10017aea5",
   "metadata": {},
   "source": [
    "## Parallel Composition\n",
    "\n",
    "The second important property of differential privacy is called *parallel composition*. \n",
    "\n",
    "Parallel composition can be seen as an alternative to sequential composition - a second way to calculate a bound on the total privacy cost of multiple data releases. \n",
    "\n",
    "Parallel composition is based on the idea of **splitting** your dataset into disjoint chunks and running a \n",
    "differentially private mechanism on each chunk separately. \n",
    "\n",
    "Since the chunks are **disjoint**, each individual's data appears in *exactly* one chunk - so even if there are $k$ chunks in total (and therefore $k$ runs of the mechanism), the mechanism runs exactly once on the data of each *individual*. \n",
    "\n",
    "Formally,\n",
    " - If $F(x)$ satisfies $\\epsilon$-differential privacy\n",
    " - And we split a dataset $X$ into $k$ disjoint chunks such that $x_1 \\cup ... \\cup x_k = X$\n",
    " - Then the mechanism which releases all of the results $F(x_1), ..., F(x_k)$ satisfies $\\epsilon$-differential privacy\n",
    "\n",
    "Note that this is a much better bound than sequential composition would give. \n",
    "\n",
    "Since we run $F$ $k$ times, sequential composition would say that this procedure satisfies $k\\epsilon$-differential privacy. \n",
    "\n",
    "Parallel composition allows us to say that the total privacy cost is just $\\epsilon$.\n",
    "\n",
    "The formal definition matches up with our intuition - if each participant in the dataset contributes one row to $X$, then this row will appear in *exactly* one of the chunks $x_1, ..., x_k$. \n",
    "\n",
    "That means $F$ will only \"see\" this participant's data *one time*, meaning a privacy cost of $\\epsilon$ is appropriate for that individual. Since this property holds for all individuals, the privacy cost is $\\epsilon$ for everyone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67375447",
   "metadata": {},
   "source": [
    "## Post-processing\n",
    "\n",
    "The third property of differential privacy is called *post-processing*. \n",
    "\n",
    "The idea is simple: it's impossible to **reverse the privacy protection** provided by differential privacy by post-processing the data in some way. \n",
    "\n",
    "Formally:\n",
    "\n",
    "- If $F(X)$ satisfies $\\epsilon$-differential privacy\n",
    "- Then for any (deterministic or randomized) function $g$, $g(F(X))$ satisfies $\\epsilon$-differential privacy\n",
    "\n",
    "The post-processing property means that it's always safe to perform arbitrary computations on the output of a differentially private mechanism - there's no danger of reversing the privacy protection the mechanism has provided. \n",
    "\n",
    "In particular, it's fine to perform post-processing that might reduce the noise or improve the signal in the mechanism's output (e.g. replacing negative results with zeros, for queries that shouldn't return negative results). \n",
    "\n",
    "The other implication of the **post-processing** property is that differential privacy provides resistance against privacy attacks based on **auxiliary information**. \n",
    "\n",
    "For example, the function $g$ might contain auxiliary information about elements of the dataset, and attempt to perform a linkage attack using this information. The post-processing property says that such an attack is limited in its effectiveness by the privacy parameter $\\epsilon$, regardless of the auxiliary information contained in $g$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc67cade-85d7-4494-bb5c-c045b414df84",
   "metadata": {},
   "source": [
    "### Histograms\n",
    "\n",
    "In our context, a *histogram* is an analysis of a dataset which splits the dataset into \"bins\" based on the value of one of the data attributes, and **counts** the number of rows in each bin. \n",
    "\n",
    "For example, a histogram might count the number of people in the dataset who achieved a particular educational level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b23c875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720f3805-ebbf-476b-988f-711cb70ed47c",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "DATASET_URL = \"https://raw.githubusercontent.com/uvm-plaid/programming-dp/master/notebooks/adult_with_pii.csv\"\n",
    "adult = pd.read_csv(DATASET_URL)\n",
    "\n",
    "adult['Education'].value_counts().to_frame().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15d1d8e-0399-4f78-9f23-aa1f669ae035",
   "metadata": {},
   "source": [
    "Histograms are particularly interesting for differential privacy because they automatically satisfy parallel composition. \n",
    "\n",
    "Each \"bin\" in a histogram is defined by a possible value for a data attribute (for example, `'Education' == 'HS-grad'`). \n",
    "\n",
    "It's impossible for a single row to have *two* values for an attribute simultaneously, so defining the bins this way *guarantees* that they will be disjoint. \n",
    "\n",
    "Thus we have satisfied the requirements for parallel composition, and we can use a differentially private mechanism to release *all* \n",
    "of the bin counts with a total privacy cost of just $\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d2c3a3-2cda-4274-9a8d-488e45b8b69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1\n",
    "sensitivity = 1\n",
    "\n",
    "# This analysis has a total privacy cost of epsilon = 1, even though we release many results!\n",
    "f = lambda x: x + np.random.laplace(loc=0, scale=sensitivity/epsilon)\n",
    "s = adult['Education'].value_counts().apply(f)\n",
    "s.to_frame().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705f4831-bb84-4929-b396-4aaed7204490",
   "metadata": {},
   "source": [
    "## Optional: Sensitivity and Clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ee6158-2850-46e1-8986-1bf96b7ee87e",
   "metadata": {},
   "source": [
    "### Sensitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39346eee-d5bb-4707-b6c6-5ccf9aea56e4",
   "metadata": {},
   "source": [
    "When discussing the Laplace mechanism, we mentioned that the amount of **noise** that is necessary to ensure differential privacy for \n",
    "a given query depends on the *sensitivity* of the query. \n",
    "\n",
    "Roughly speaking, the sensitivity of a function reflects the amount the function's output will change when its input changes. \n",
    "\n",
    "Recall that the Laplace mechanism defines a mechanism $F(x)$ as follows:\n",
    "\n",
    "\\begin{align}\n",
    "F(x) = f(x) + \\textsf{Lap}\\left(\\frac{s}{\\epsilon}\\right)\n",
    "\\end{align}\n",
    "\n",
    "where $f(x)$ is a deterministic function (the query), $\\epsilon$ is the privacy parameter, and $s$ is the sensitivity of $f$.\n",
    "\n",
    "For a function $f : \\mathcal{D} \\rightarrow \\mathbb{R}$ mapping datasets ($\\mathcal{D}$) to real numbers, the *global sensitivity* of $f$ is defined as follows:\n",
    "\n",
    "\\begin{align}\n",
    "GS(f) = \\max_{x, x': d(x,x') <= 1} |f(x) - f(x')|\n",
    "\\end{align}\n",
    "\n",
    "Here, $d(x, x')$ represents the *distance* between two datasets $x$ and $x'$, and we say that two datasets are *neighbors* if their distance is 1 or less. \n",
    "\n",
    "How this distance is defined has a huge effect on the definition of privacy we obtain.\n",
    "\n",
    "The definition of global sensitivity says that for *any two* neighboring datasets $x$ and $x'$, the difference between $f(x)$ and $f(x')$ is at most $GS(f)$. \n",
    "\n",
    "**Global vs Local Sensitivity**:\n",
    "\n",
    "This measure of sensitivity is called \"global\" because it is independent of the actual dataset being queried (it holds for *any* choice of neighboring $x$ and $x'$). \n",
    "\n",
    "Another measure of sensitivity, called *local sensitivity*, fixes one of the datasets to be the one being queried."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ecc7dc-36d6-4c3c-bdbd-f8a162366609",
   "metadata": {},
   "source": [
    "### Distance\n",
    "\n",
    "The distance metric $d(x,x')$ described earlier can be defined in many different ways. \n",
    "\n",
    "Intuitively, the distance between two datasets should be equal to 1 (i.e. the datasets are neighbors) if they differ in the data of exactly **one individual**. \n",
    "\n",
    "This idea is easy to formalize in some contexts (e.g. in the US Census, each individual submits a single response containing their data) but extremely challenging in others (e.g. location trajectories, social networks, and time-series data).\n",
    "\n",
    "A common formal definition for datasets containing rows is to consider the number of rows which differ between the two. \n",
    "\n",
    "When each individual's data is contained in a single row, this definition often makes sense. \n",
    "\n",
    "Formally, this definition of distance is encoded as a **symmetric difference** between the two datasets:\n",
    "\n",
    "\\begin{align}\n",
    "d(x, x') = | x - x' \\cup x' - x |\n",
    "\\end{align}\n",
    "\n",
    "This particular definition has several interesting and important implications:\n",
    "- If $x'$ is constructed from $x$ by *adding one row*, then $d(x,x') = 1$\n",
    "- If $x'$ is constructed from $x$ by *removing one row*, then $d(x,x') = 1$\n",
    "- If $x'$ is constructed from $x$ by *modifying one row*, then $d(x,x') = 2$\n",
    "\n",
    "In other words, adding or removing a row results in a neighboring dataset; *modifying* a row results in a dataset at distance *2*. \n",
    "\n",
    "This particular definition of distance results in what is typically called *unbounded differential privacy*. Many other definitions are possible, including one called **bounded differential privacy** in which modifying a single row in a dataset *does* result in a neighboring dataset. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d620b327-8fd8-4c71-9110-6595c0e68667",
   "metadata": {},
   "source": [
    "#### Calculating Sensitivity\n",
    "\n",
    "How do we determine the sensitivity of a particular function of interest? For some simple functions on real numbers, the answer is obvious.\n",
    "\n",
    "- The global sensitivity of $f(x) = x$ is 1, since changing $x$ by 1 changes $f(x)$ by 1\n",
    "- The global sensitivity of $f(x) = x+x$ is 2, since changing $x$ by 1 changes $f(x)$ by 2\n",
    "- The global sensitivity of $f(x) = 5*x$ is 5, since changing $x$ by 1 changes $f(x)$ by 5\n",
    "- The global sensitivity of $f(x) = x*x$ is unbounded, since the change in $f(x)$ depends on the value of $x$\n",
    "\n",
    "For functions that map datasets to real numbers, we can perform a similar analysis. We will consider the functions which represent common aggregate database queries: counts, sums, and averages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f401d8-c5e6-409c-9973-6604bac4ef38",
   "metadata": {},
   "source": [
    "#### Counting Queries\n",
    "\n",
    "Counting queries (`COUNT` in SQL) count the number of rows in the dataset which satisfy a specific property. \n",
    "\n",
    "As a rule of thumb, **counting queries always have a sensitivity of 1**. \n",
    "\n",
    "This is because adding a row to the dataset can increase the output of the query by at most 1: either the new row has the desired property, and the count increases by 1, or it does not, and the count stays the same (the count may correspondingly decrease when a row is removed)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dc86d5-b81c-4173-bc05-2625cfc083d3",
   "metadata": {},
   "source": [
    "**Example: \"How many people are in the dataset?\"** (sensitivity: 1 - counting rows where the property = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f12bca-afa9-482d-bf36-9b8b3577a67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "adult.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ddef18-456a-4f87-999b-b7bc4b3129d9",
   "metadata": {},
   "source": [
    "**Example: \"How many people have an educational status above 10?\"** (sensitivity: 1 - counting rows with a property)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4399cb53-cfd6-4931-bf0c-66a344d67a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "adult[adult['Education-Num'] > 10].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635a58cb-4853-4178-8717-61a28688dce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "adult[adult['Name'] == 'Joe Near'].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faaef93-ce65-4446-969d-2eb16920789f",
   "metadata": {},
   "source": [
    "#### Summation Queries\n",
    "\n",
    "Summation queries (`SUM` in SQL) sum up the *attribute values* of dataset rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea93f080-7116-4265-856d-4bd3c30e4838",
   "metadata": {},
   "source": [
    "**Example: \"What is the sum of the ages of people with an educational status above 10?\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d62a9e8-2024-4844-91f3-404890a7f124",
   "metadata": {},
   "outputs": [],
   "source": [
    "adult[adult['Education-Num'] > 10]['Age'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37f827f-233a-4038-aae6-d4f1857e8744",
   "metadata": {},
   "source": [
    "Sensitivity for these queries is not **as simple as it is for counting queries**. \n",
    "\n",
    "Adding a new row to the dataset will increase the result of our example query by the *age of the new person*. \n",
    "\n",
    "That means the sensitivity of the query depends on the **contents** of the row we add.\n",
    "\n",
    "As a rule of thumb, summation queries have **unbounded sensitivity** when no lower and upper bounds exist on the value of the attribute being summed. \n",
    "\n",
    "When lower and upper bounds do exist, the sensitivity of a summation query is equal to the **difference between them**. \n",
    "\n",
    "In the next section, we will see a technique called **clipping** for enforcing bounds when none exist, so that summation queries with unbounded sensitivity can be converted into queries with bounded sensitivity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8851ab7-acb0-4155-b6a0-480a8bb5312e",
   "metadata": {},
   "source": [
    "#### Average Queries\n",
    "\n",
    "Average queries (`AVG` in SQL) calculate the mean of attribute values in a particular column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb73ae9a-26cd-4e9e-b303-64f18e404e37",
   "metadata": {},
   "source": [
    "**Example: \"What is the average age of people with an educational status above 10?\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbd47bf-8509-462e-9138-7b3dc57d95c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "adult[adult['Education-Num'] > 10]['Age'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b79a76-2e13-420a-a3dc-7387f44268a2",
   "metadata": {},
   "source": [
    "The easiest way to answer an average query with differential privacy is by re-phrasing it as two queries: a summation query divided by a counting query. For the above example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cda7c33-ce63-472a-89d3-2f60fec93839",
   "metadata": {},
   "outputs": [],
   "source": [
    "adult[adult['Education-Num'] > 10]['Age'].sum() / adult[adult['Education-Num'] > 10]['Age'].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9263f787-cb9a-4d82-baa8-57d333abe6a0",
   "metadata": {},
   "source": [
    "The sensitivities of both queries can be calculated as described above. \n",
    "\n",
    "Noisy answers for each can be calculated (e.g. using the Laplace mechanism) and the noisy answers can be divided to obtain a differentially private mean. \n",
    "\n",
    "The total privacy cost of both queries can be calculated by **sequential composition**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497056eb-09a5-4336-bf25-13387de886dd",
   "metadata": {},
   "source": [
    "### Clipping\n",
    "\n",
    "Queries with unbounded sensitivity cannot be directly answered with differential privacy using the Laplace mechanism. \n",
    "\n",
    "Fortunately, we can often transform such queries into equivalent queries with *bounded* sensitivity, via a process called **clipping**.\n",
    "\n",
    "The basic idea behind clipping is to **enforce** upper and lower bounds on attribute values. \n",
    "\n",
    "> For example, ages above 125 can be \"clipped\" to exactly 125. \n",
    "\n",
    "After clipping has been performed, we are **guaranteed** that all ages will be 125 or below. \n",
    "\n",
    "As a result, the sensitivity of a summation query on clipped data is equal to the difference between the upper and lower bounds used in clipping: $upper - lower$. \n",
    "\n",
    "For example, the following query has a sensitivity of 125:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721081ec-c922-4551-83b5-6317ec12533b",
   "metadata": {},
   "outputs": [],
   "source": [
    "adult['Age'].clip(lower=0, upper=125).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdb4402-0310-40f9-91db-ca6af05e3f86",
   "metadata": {},
   "source": [
    "The primary challenge in performing clipping is to determine the **upper** and **lower** bounds. \n",
    "\n",
    "Furthermore, there is a tradeoff between the amount of information lost in clipping and the amount of noise needed to ensure differential privacy. \n",
    "\n",
    "As a rule of thumb, **try to set the clipping bounds to include 100% of the dataset**, or get as close as possible. This is harder in some domains (e.g. graph queries) than others.\n",
    "\n",
    "It's tempting to determine the clipping bounds by looking at the data. For example, we can look at the histogram of ages in our dataset to determine an appropriate upper bound:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b94469-12ec-4ab9-a021-ea8fa52a2100",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.hist(adult['Age'])\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Number of Records');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4289b910-111b-468e-b726-ddf7c0722ecb",
   "metadata": {},
   "source": [
    "It's clear from this histogram that nobody in this particular dataset is over 90, so an upper bound of 90 would suffice.\n",
    "\n",
    "**NOTE**: However, it's important to note that **this approach does not satisfy differential privacy**. \n",
    "\n",
    "If we pick our clipping bounds by looking at the data, then the bounds themselves might reveal something about the data.\n",
    "\n",
    "Typically, clipping bounds are decided either by using a property of the dataset that can be known without looking at the data (e.g. that the dataset contains ages, which are likely to lie between 0 and 125), or by performing **differentially private queries** to evaluate different choices for the clipping bounds.\n",
    "\n",
    "**Determine Upper bound with differentially private queries**:\n",
    "\n",
    "To use the second approach, we typically set the lower bound to 0 and slowly increase the upper bound until the query's output stops changing (meaning we haven't included any new data by increasing the bound). \n",
    "\n",
    "For example, let's try computing the sum of ages for clipping bounds `from 0 to 100``, using the Laplace mechanism for each one to ensure differential privacy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fc38a2-4b32-4f7a-afdd-8b01a4ad52b2",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def laplace_mech(v, sensitivity, epsilon):\n",
    "    return v + np.random.laplace(loc=0, scale=sensitivity/epsilon)\n",
    "\n",
    "epsilon_i = .01\n",
    "plt.plot([laplace_mech(adult['Age'].clip(lower=0, upper=i).sum(), i, epsilon_i) for i in range(100)])\n",
    "plt.xlabel('Clipping Bound for Age')\n",
    "plt.ylabel('Total Sum');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb09538c-800b-4294-b59b-24dacd48aab4",
   "metadata": {},
   "source": [
    "The total privacy cost for building this plot is $\\epsilon = 1$ by sequential composition, since we do 100 queries each with $\\epsilon_i = 0.01$. It's clear that the results level off around a value of `upper = 80`, so this is a good choice for the clipping bound.\n",
    "\n",
    "One refinement that **can work well when the scale of the data is not known** is to test upper bounds according to a logarithmic scale."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
