{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b16a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: setup NoTexBook theme\n",
    "%load_ext notexbook\n",
    "\n",
    "%texify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ccc63d",
   "metadata": {},
   "source": [
    "# Federated Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b0ea14",
   "metadata": {},
   "source": [
    "![](https://blog.openmined.org/content/images/2020/02/0.PNG)\n",
    "\n",
    "**Source**: [Open Mined Blog](https://blog.openmined.org/content/images/2020/02/0.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b9fcac",
   "metadata": {},
   "source": [
    "## Is Federated Learning similar to Distributed Computation?\n",
    "\n",
    "**tldr;** **NO**\n",
    "\n",
    "- Client devices (such as smartphones) have limited network bandwidth. They cannot transfer large amounts of data and the upload speed is usually lower than the download speed.\n",
    "- The client devices are not always available to take part in a training session. Optimal conditions such as charging state, connection to an unmetered Wi-Fi network, idleness, etc. are not always achievable.\n",
    "- The data present on the device get updated quickly and is not always the same. [Data is not always available.]\n",
    "- The client devices can choose not to participate in the training.\n",
    "- The number of client devices available is very large but inconsistent.\n",
    "- Federated learning incorporates privacy preservation with distributed training and aggregation across a large population.\n",
    "- The data is usually unbalanced as the data is user-specific and is self-correlated.\n",
    "- Federated Learning is one instance of the more general approach of “bringing the code to the data, instead of the data to the code” and addresses the fundamental problems of privacy, ownership, and locality of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110d61fd",
   "metadata": {},
   "source": [
    "### In Federated Learning:\n",
    "\n",
    "- Certain techniques are used to compress the model updates.\n",
    "- Quality updates are performed rather than simple gradient steps.\n",
    "- Noise is added by the server before performing aggregation to obscure the impact of an individual on the learned model. [Global Differential Privacy]\n",
    "- The gradients updates are clipped if they are too large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6245cbbf",
   "metadata": {},
   "source": [
    "![](https://camo.githubusercontent.com/3c74785be74fa72f34b059b5b7333845ed1dfebc/68747470733a2f2f626c6f672e6f70656e6d696e65642e6f72672f636f6e74656e742f696d616765732f323032302f30342f4f4d2d2d2d434b4b532d477261706869632d762e30314032782e706e67)\n",
    "\n",
    "**Source**: [PySyft](https://github.com/OpenMined/PySyft/blob/syft_0.5.0/packages/syft/examples/homomorphic-encryption/Tutorial_0_TenSEAL_Syft_Data_Scientist.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d717eee2",
   "metadata": {
    "tags": []
   },
   "source": [
    "**What About Encryption?**\n",
    "\n",
    "### Introducing `HE`: Homomorphic Encryption\n",
    "\n",
    "> **Definition**:\n",
    "> \n",
    "> Homomorphic encryption (`HE`) is a technique that enables computations on encrypted data **without** decrypting the actual data. Moreover, any operation made on ciphertexts, generates results that when decrypted, corresponds to the result of the same computations made on plaintexts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
