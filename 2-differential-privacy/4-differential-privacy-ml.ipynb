{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: setup NoTexBook theme\n",
    "%load_ext notexbook\n",
    "%texify -fs 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adapted from**: [Ch12](https://github.com/uvm-plaid/programming-dp/blob/master/notebooks/ch12.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differentially Private Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Some useful utilities\n",
    "\n",
    "def laplace_mech(v, sensitivity, epsilon):\n",
    "    return v + np.random.laplace(loc=0, scale=sensitivity / epsilon)\n",
    "\n",
    "def gaussian_mech(v, sensitivity, epsilon, delta):\n",
    "    return v + np.random.normal(loc=0, scale=sensitivity * np.sqrt(2*np.log(1.25/delta)) / epsilon)\n",
    "\n",
    "def gaussian_mech_vec(v, sensitivity, epsilon, delta):\n",
    "    return v + np.random.normal(loc=0, scale=sensitivity * np.sqrt(2*np.log(1.25/delta)) / epsilon, size=len(v))\n",
    "\n",
    "def pct_error(orig, priv):\n",
    "    return np.abs(orig - priv)/orig * 100.0\n",
    "\n",
    "def z_clip(xs, b):\n",
    "    return [min(x, b) for x in xs]\n",
    "\n",
    "def g_clip(v):\n",
    "    n = np.linalg.norm(v, ord=2)\n",
    "    if n > 1:\n",
    "        return v / n\n",
    "    else:\n",
    "        return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will explore how building differentially private machine learning classifiers. \n",
    "\n",
    "We'll focus on a kind of *supervised learning* problem: given a set of *labeled training examples* \n",
    "$\\{(x_1, y_1), \\dots, (x_n, y_n)\\}$, in which $x_i$ is called the *feature vector* and $y_i$ is called the *label*, train a *model* $\\theta$ which can *predict* the label for a new feature vector which was not present in the training set. \n",
    "\n",
    "Each $x_i$ is typically a vector of real numbers which describe the features of a training example, and the $y_i$s are drawn from a predefined set of *classes* (usually expressed as integers) that examples can be drawn from.\n",
    "\n",
    "A *binary* classifier has two classes (usually either 1 and 0, or 1 and -1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "The dataset files you'll need are available here:\n",
    "\n",
    "- [`adult_processed_x`](https://github.com/uvm-plaid/programming-dp/raw/master/notebooks/adult_processed_x.npy)\n",
    "- [`adult_processed_y`](https://github.com/uvm-plaid/programming-dp/raw/master/notebooks/adult_processed_y.npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-07-10 05:41:57--  https://github.com/uvm-plaid/programming-dp/raw/master/notebooks/adult_processed_x.npy\n",
      "Resolving github.com (github.com)... 140.82.112.3\n",
      "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/uvm-plaid/programming-dp/master/notebooks/adult_processed_x.npy [following]\n",
      "--2023-07-10 05:41:57--  https://raw.githubusercontent.com/uvm-plaid/programming-dp/master/notebooks/adult_processed_x.npy\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 37623120 (36M) [application/octet-stream]\n",
      "Saving to: ‘adult_processed_x.npy.1’\n",
      "\n",
      "adult_processed_x.n 100%[===================>]  35.88M  10.3MB/s    in 3.6s    \n",
      "\n",
      "2023-07-10 05:42:02 (10.1 MB/s) - ‘adult_processed_x.npy.1’ saved [37623120/37623120]\n",
      "\n",
      "--2023-07-10 05:42:02--  https://github.com/uvm-plaid/programming-dp/raw/master/notebooks/adult_processed_y.npy\n",
      "Resolving github.com (github.com)... 140.82.112.3\n",
      "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/uvm-plaid/programming-dp/master/notebooks/adult_processed_y.npy [following]\n",
      "--2023-07-10 05:42:02--  https://raw.githubusercontent.com/uvm-plaid/programming-dp/master/notebooks/adult_processed_y.npy\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 361840 (353K) [application/octet-stream]\n",
      "Saving to: ‘adult_processed_y.npy’\n",
      "\n",
      "adult_processed_y.n 100%[===================>] 353.36K  --.-KB/s    in 0.09s   \n",
      "\n",
      "2023-07-10 05:42:03 (3.98 MB/s) - ‘adult_processed_y.npy’ saved [361840/361840]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/uvm-plaid/programming-dp/raw/master/notebooks/adult_processed_x.npy\n",
    "!wget https://github.com/uvm-plaid/programming-dp/raw/master/notebooks/adult_processed_y.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "X = np.load('adult_processed_x.npy')\n",
    "y = np.load('adult_processed_y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9044,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_size = int(X.shape[0] * 0.8)\n",
    "\n",
    "X_train = X[:training_size]\n",
    "X_test = X[training_size:]\n",
    "\n",
    "y_train = y[:training_size]\n",
    "y_test = y[training_size:]\n",
    "\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple way to build a binary classifier is with *logistic regression*. The scikit-learn library has a built-in module for performing logistic regression, called `LogisticRegression`, and it's easy to use to build a model using our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression().fit(X_train[:1000],y_train[:1000])\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can use the model's `predict` method to predict labels for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1., -1., -1., ..., -1., -1., -1.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how many test examples does our model get correct? We can compare the\n",
    "predicted labels against the actual labels from the dataset; if we divide\n",
    "the number of correctly predicted labels by the total number of test\n",
    "examples, we can measure the percent of the examples which are correctly\n",
    "classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8243034055727554"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(model.predict(X_test) == y_test)/X_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Our model predicts the correct label for 82% of the examples in our test\n",
    "set. For this dataset, that's a pretty decent result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Model?\n",
    "\n",
    "What exactly *is* a model? How does it encode the information it uses to make predictions?\n",
    "\n",
    "There are many different kinds of models, but the ones we'll explore here are *linear models*. For an unlabeled example with a $k$-dimensional feature vector $x_1, \\dots, x_k$, a linear model predicts a label by first calculating the quantity:\n",
    "\n",
    "\\begin{align}\n",
    "w_1 x_1 + \\dots + w_k x_k + bias\n",
    "\\end{align}\n",
    "\n",
    "and then taking the sign of it (i.e. if the quantity above is negative, we predict the label -1; if it's positive, we predict 1).\n",
    "\n",
    "The model itself, then, can be represented by a vector containing the values $w_1, \\dots, w_k$ and the value for $bias$. The model is said to be linear because the quantity we calculate in predicting a label is a polynomial of degree 1 (i.e. linear). The values $w_1, \\dots, w_k$ are often called the *weights* or *coefficients* of the model, and $bias$ is often called the *bias term* or *intercept*.\n",
    "\n",
    "This is actually how scikit-learn represents its logistic regression model, too! We can check out the weights of our trained model using the `coef_` attribute of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true,
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.346167750303831,\n",
       " array([ 3.76035057e-01, -2.55358856e-01, -3.21341426e-02,  3.74545737e-01,\n",
       "        -6.85885223e-01,  3.91875239e-01, -1.69476241e-01, -7.41793527e-02,\n",
       "        -5.76496538e-01,  3.94976503e-01, -3.41457312e-01, -6.24912317e-01,\n",
       "        -6.05605602e-01, -4.56928100e-01, -5.19167009e-01, -1.05743009e-01,\n",
       "         8.19586633e-01,  9.96762702e-01, -3.09342985e-01,  6.57277160e-01,\n",
       "        -1.06436104e-01,  7.71287796e-01,  7.99791034e-02,  1.43803702e-01,\n",
       "        -1.01006564e-01,  1.59416785e+00, -5.06233997e-02, -5.78477239e-01,\n",
       "        -3.72601413e-01, -6.35661364e-01, -1.02810175e-01,  0.00000000e+00,\n",
       "        -1.35478173e-01,  4.36864993e-01, -3.42554362e-01, -1.32819675e-01,\n",
       "        -2.00200285e-01, -1.53919241e+00,  6.44831702e-02,  7.17836796e-01,\n",
       "         3.80039408e-01,  4.25898498e-02,  8.81653483e-01, -7.08110462e-02,\n",
       "         6.10385977e-02,  8.94590966e-02,  6.93679716e-01, -1.30382712e+00,\n",
       "        -6.55878656e-01,  1.11512993e+00,  3.78012650e-01, -4.28231712e-02,\n",
       "        -3.72812689e-01,  2.41180415e-01, -2.03955636e-01, -3.07042908e-01,\n",
       "         3.06644477e-01,  4.31360344e-01,  5.31199745e-01, -6.89615763e-02,\n",
       "         4.66366585e-01, -5.81829004e-01, -2.21952424e-01, -2.39529124e-01,\n",
       "        -1.40562769e-03,  7.26045748e-01,  2.46167426e-01, -6.08617054e-01,\n",
       "         0.00000000e+00, -9.02427102e-02, -3.54430134e-03,  0.00000000e+00,\n",
       "         0.00000000e+00, -1.29034794e-01,  5.90856998e-01, -5.15912614e-01,\n",
       "         0.00000000e+00, -5.42096249e-03,  7.28556009e-01, -5.15261422e-02,\n",
       "         2.30704112e-01, -1.61821068e-01, -6.60183260e-01, -1.01170807e-01,\n",
       "        -2.52337853e-01, -5.77230791e-02, -1.45064565e-01, -3.09985224e-01,\n",
       "         0.00000000e+00, -3.31415590e-02,  0.00000000e+00, -1.38495395e-01,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  4.26243747e-01,\n",
       "         0.00000000e+00,  0.00000000e+00,  1.72867533e+00,  7.37281346e-02,\n",
       "         1.83154145e+00,  2.40009511e+00,  1.46921214e+00,  1.96856497e+00]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.intercept_[0], model.coef_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we'll always have exactly the same number of weights $w_i$ as we have features $x_i$, since we have to multiply each feature by its corresponding weight. That means our model has exactly the same dimensionality as our feature vectors.\n",
    "\n",
    "Now that we have a way to get the weights and bias term, we can implement our own function to perform prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8243034055727554"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction: take a model (theta) and a single example (xi) and return its predicted label\n",
    "def predict(xi, theta, bias=0):\n",
    "    label = np.sign(xi @ theta + bias)\n",
    "    return label\n",
    "\n",
    "np.sum(predict(X_test, model.coef_[0], model.intercept_[0]) == y_test)/X_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've made the bias term optional here, because in many cases it's possible to do just as well without it. \n",
    "\n",
    "To make things simpler, we won't bother to train a bias term in our own algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Model with Gradient Descent\n",
    "\n",
    "The scikit-learn library has some pretty sophisticated algorithms, but we can do just about as well by implementing a simple and very popular method called **gradient descent**.\n",
    "\n",
    "> **Note**: A similar version of this algorithms is used when training deep neural networks!\n",
    "\n",
    "Most training algorithms for machine learning are defined in terms of a **loss function**, which specifies a way to measure how \"bad\" a model is at prediction. \n",
    "\n",
    "The goal of the training algorithm is to minimize the output of the loss function - a model with low loss will be *good* at prediction.\n",
    "\n",
    "There exists many loss functions. A commonly used loss function for **binary classification** is the **logistic loss**:\n",
    "the logistic loss gives us a measure of \"how far\" we are from predicting the correct label.\n",
    "\n",
    "The logistic loss is implemented by the following Python function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The loss function measures how good our model is. The training goal is to minimize the loss.\n",
    "# This is the logistic loss function.\n",
    "def loss(theta, xi, yi):\n",
    "    exponent = - yi * (xi.dot(theta))\n",
    "    return np.log(1 + np.exp(exponent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the loss function to measure how good a particular model is. \n",
    "\n",
    "Let's try it out with a model whose weights are all zeros. This model isn't likely to work very well, but it's a starting point from which we can train a better one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6931471805599453"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = np.zeros(X_train.shape[1])\n",
    "loss(theta, X_train[0], y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We typically measure how good our model is over our entire training set by simply averaging the loss over all of the examples in the training data. \n",
    "\n",
    "In this case, we get *every* example wrong, so the average loss on the whole training set is exactly equal to the loss we calculated above for just one example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6931471805599453"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([loss(theta, x_i, y_i) for x_i, y_i in zip(X_train, y_train)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal in *training* the model is to *minimize* the loss. \n",
    "\n",
    "So the key question is: how do we modify the model to make the loss smaller? \n",
    "\n",
    "Gradient descent is an approach that makes the loss smaller by updating the model according to the [*gradient*](https://en.wikipedia.org/wiki/Gradient) of the loss. \n",
    "\n",
    "If the gradient is positive in a particular dimension, that means the function's value will *increase* if we increase the model's weight for that dimension; we want the loss to *decrease*, so we should modify our model by *negating* the gradient - i.e. do the *opposite* of what the gradient says. Since we move the model in the opposite direction of the gradient, this is called *descending* the gradient.\n",
    "\n",
    "When we iteratively perform many steps of this descent process, we slowly get closer and closer to the model which minimizes the loss. \n",
    "\n",
    "This algorithm is called **gradient descent**.\n",
    "\n",
    "Let's see how this looks in Python; first, we'll define the gradient function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the gradient of the logistic loss\n",
    "# The gradient is a vector that indicates the rate of change of the loss in each direction\n",
    "def gradient(theta, xi, yi):\n",
    "    exponent = yi * (xi.dot(theta))\n",
    "    return - (yi*xi) / (1+np.exp(exponent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Single Step of Gradient Descent\n",
    "\n",
    "Next, let's perform a single step of gradient descent. We can apply the `gradient` function to a single example from our training data, which should give us enough information to improve the model for that example. We \"descend\" the gradient by subtracting it from our current model `theta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        , -0.5       ,\n",
       "        0.        ,  0.        ,  0.        , -0.5       ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "       -0.5       ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        , -0.5       ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        , -0.5       ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        , -0.5       ,\n",
       "        0.        , -0.5       ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "       -0.5       ,  0.        ,  0.        , -0.25      , -0.0606146 ,\n",
       "       -0.21875   ,  0.        ,  0.        , -0.17676768])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we take a step in the *opposite* direction from the gradient (by negating it), we should \n",
    "# move theta in a direction that makes the loss *lower*\n",
    "# This is one step of gradient descent - in each step, we're trying to \"descend\" the gradient\n",
    "# In this example, we're taking the gradient on just a single training example (the first one)\n",
    "theta = theta - gradient(theta, X_train[0], y_train[0])\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we call `predict` on the same example from the training data, its label is predicted correctly! That means our update did indeed improve the model, since it's now capable of classifying this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.0, -1.0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0], predict(theta, X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be measuring the accuracy of our model many times, so let's define a helper function for measuring accuracy. It works in the same way as the accuracy measurement for the sklearn model above. We can use it on the `theta` we've built by descending the gradient for one example, to see how good our model is on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7585139318885449"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracy(theta):\n",
    "    return np.sum(predict(X_test, theta) == y_test)/X_test.shape[0]\n",
    "\n",
    "accuracy(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our improved model now predicts 75% of the labels for the test set correctly! That's good progress - we've improved the model considerably."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Gradient Descent Algorithm\n",
    "\n",
    "**We need to make two changes to arrive at a basic gradient descent algorithm.**\n",
    "\n",
    "First, our single step above used only a single example from the training data; we want to consider the *whole* training set when updating the model, so that we improve the model for *all* examples. \n",
    "\n",
    "Second, we need to perform multiple iterations, to get as close as possible to minimizing the loss.\n",
    "\n",
    "We can solve the first problem by calculating the *average gradient* over all of the training examples, and using it for the descent step in place of the single-example gradient we used before. Our `avg_grad` function calculates the average gradient over a whole array of training examples and the corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true,
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8.03202480e-03, -1.09365062e-02, -5.86649848e-02, -1.70297784e-02,\n",
       "       -1.85949049e-02, -5.32762100e-03,  3.15432083e-05,  2.24692568e-03,\n",
       "        1.80942171e-03,  1.10891317e-03,  7.17940863e-04,  1.22012681e-03,\n",
       "        1.09385854e-03,  1.42352970e-03, -4.29266203e-03, -5.73114012e-03,\n",
       "       -4.96409990e-02, -7.90844879e-03, -1.08970068e-02, -2.50609905e-02,\n",
       "        3.27410319e-04, -1.20102580e-02, -1.29608985e-02,  1.15182321e-02,\n",
       "       -2.26895536e-04, -1.83255483e-01,  1.34642262e-03,  4.47703452e-02,\n",
       "        4.31895523e-03,  2.97414610e-03,  6.16295082e-03, -4.88903955e-05,\n",
       "       -2.13933205e-02, -4.86969833e-02, -8.62802483e-04,  3.11463168e-03,\n",
       "        1.23013848e-03,  1.54486498e-02,  1.21336873e-03, -4.38864985e-02,\n",
       "       -4.34689131e-03, -1.64743409e-02, -4.53583200e-03, -5.47845717e-03,\n",
       "       -1.67472715e-01,  1.93015718e-02,  4.73608091e-03,  2.44149704e-02,\n",
       "        1.61917788e-02, -1.57259641e-02,  6.59058497e-04, -1.58429762e-03,\n",
       "        9.21938268e-03,  8.76978910e-04, -1.27725399e-01,  3.39811988e-02,\n",
       "       -1.52535476e-01, -1.11859092e-04, -7.43481028e-04, -2.46346175e-04,\n",
       "        2.71911076e-04, -2.55366711e-04,  4.50825450e-04,  1.10378277e-04,\n",
       "        3.56606530e-04, -6.45268003e-04, -2.29994332e-04, -3.86436617e-04,\n",
       "       -3.08625397e-04,  2.96102401e-04,  1.88227302e-04,  8.58078928e-06,\n",
       "        7.20867325e-05, -4.19942412e-05, -8.78083803e-05, -8.39666492e-04,\n",
       "       -3.06575834e-04, -8.40712924e-05, -5.70563641e-04,  4.00302057e-04,\n",
       "       -2.64158094e-04,  6.99057157e-05,  2.42709304e-03,  1.82470777e-04,\n",
       "        8.76079931e-05,  1.54645694e-04, -2.72063515e-04, -6.37207436e-05,\n",
       "        1.24980547e-05,  4.45197135e-04,  4.61621071e-05,  1.15265174e-04,\n",
       "       -2.77439358e-04,  5.96595409e-05,  1.20539191e-04, -1.18965672e-01,\n",
       "        3.44932395e-04, -7.41634269e-05, -6.91870325e-02, -1.45516103e-02,\n",
       "       -9.95735544e-02, -8.85669054e-03, -9.10018120e-03, -6.35462985e-02])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def avg_grad(theta, X, y):\n",
    "    grads = [gradient(theta, xi, yi) for xi, yi in zip(X, y)]\n",
    "    return np.mean(grads, axis=0)\n",
    "\n",
    "avg_grad(theta, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve the second problem, we'll define an iterative algorithm that descends the gradient multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(iterations):\n",
    "    # Start by \"guessing\" what the model should be (all zeros)\n",
    "    theta = np.zeros(X_train.shape[1])\n",
    "\n",
    "    # Perform `iterations` steps of gradient descent using training data\n",
    "    for i in range(iterations):\n",
    "        theta = theta - avg_grad(theta, X_train, y_train)\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7787483414418399"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = gradient_descent(10)\n",
    "accuracy(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 10 iterations, our model reaches nearly 78% accuracy - not bad! \n",
    "\n",
    "Our gradient descent algorithm looks simple (and it is!) but don't let its simplicity fool you - this basic approach is behind many of the recent successes in large-scale deep learning, and our algorithm is *very* close in its design to the ones implemented in popular frameworks for machine learning like PyTorch.\n",
    "\n",
    "Notice that we didn't quite make it to the 84% accuracy of the sklearn model we trained earlier. \n",
    "\n",
    "Don't worry - our algorithm is definitely capable of this! \n",
    "\n",
    "**We just need more iterations, to get closer to the minimum of the loss.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 100 iterations, we get closer - 82% accuracy. \n",
    "\n",
    "However, the algorithm takes a long time to run when we ask for so many iterations. Even worse, the closer we get to minimizing the loss, the more difficult it is to improve. \n",
    "\n",
    "Also as we perform more iterations, we slowly get closer to minimizing the loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.549109439168421\n",
      "Testing loss: 0.5415350837580458\n",
      "\n",
      "Training loss: 0.5224689105514977\n",
      "Testing loss: 0.5162665121068426\n",
      "\n",
      "Training loss: 0.5028090736020403\n",
      "Testing loss: 0.49753785424732383\n",
      "\n",
      "Training loss: 0.4878874803989895\n",
      "Testing loss: 0.48335633696635527\n",
      "\n",
      "Training loss: 0.47628573924997925\n",
      "Testing loss: 0.4723742456095848\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def gradient_descent_log(iterations):\n",
    "    theta = np.zeros(X_train.shape[1])\n",
    "\n",
    "    for i in range(iterations):\n",
    "        theta = theta - avg_grad(theta, X_train, y_train)\n",
    "        print(f'Training loss: {np.mean(loss(theta, X_train, y_train))}')\n",
    "        print(f'Testing loss: {np.mean(loss(theta, X_test, y_test))}\\n')\n",
    "\n",
    "    return theta\n",
    "\n",
    "gradient_descent_log(5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also note that the training and testing loss are very close to one another, suggesting that our model is not *overfitting* to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent with Differential Privacy\n",
    "\n",
    "How can we make the above **algorithm differentially private**? \n",
    "\n",
    "We'd like to design an algorithm that ensures differential privacy for the training data, so that the final model doesn't reveal anything about individual training examples.\n",
    "\n",
    "The only part of the algorithm which uses the training data is the gradient calculation. \n",
    "\n",
    "**One way to make the algorithm differentially private is to add noise to the gradient itself** at each iteration before updating the model. This approach is usually called *noisy gradient descent*, since we add noise directly to the gradient.\n",
    "\n",
    "Our gradient function is a **vector-valued function**, so we can use `gaussian_mech_vec` to add noise to its output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noisy_gradient_descent(iterations, epsilon, delta):\n",
    "    theta = np.zeros(X_train.shape[1])\n",
    "    sensitivity = '???'\n",
    "\n",
    "    for i in range(iterations):\n",
    "        grad = avg_grad(theta, X_train, y_train)\n",
    "        noisy_grad = gaussian_mech_vec(grad, sensitivity, epsilon, delta)\n",
    "        theta = theta - noisy_grad\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's just one piece of the puzzle missing:\n",
    "\n",
    "**what is the sensitivity of the gradient function**? \n",
    "\n",
    "Answering this question is the central difficulty in making the algorithm work.\n",
    "\n",
    "There are two major challenges here:\n",
    "\n",
    "1. The gradient is the result of an *average query* \n",
    "    - it's the mean of many per-example gradients. \n",
    "    - As we've seen previously, it's best to split queries like this up into a sum query and a count query. \n",
    "    - This isn't difficult to do: we can compute the noisy sum of the per-example gradients, rather than their average, and divide by a noisy count later.\n",
    "\n",
    "2. We need to **bound** the sensitivity of each per-example gradient. \n",
    "    - There are two basic approaches for this: we can either analyze the gradient function itself (as we have done with previous queries) to determine its worst-case global sensitivity, or we can *enforce* a sensitivity by clipping the output of the gradient function (as we did in sample and aggregate).\n",
    "\n",
    "We'll start with the second approach - often called **gradient clipping** - because it's simpler conceptually and more general in its applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Clipping\n",
    "\n",
    "Recall that when we implemented sample and aggregate, we enforced a desired sensitivity on a function $f$ with unknown sensitivity by clipping its output. \n",
    "\n",
    "The sensitivity of $f$ was:\n",
    "\n",
    "\\begin{align}\n",
    "\\lvert f(x) - f(x') \\rvert\n",
    "\\end{align}\n",
    "\n",
    "After clipping with parameter $b$, this becomes:\n",
    "\n",
    "\\begin{align}\n",
    "\\lvert \\mathsf{clip}(f(x), b) - \\mathsf{clip}(f(x'),b) \\rvert\n",
    "\\end{align}\n",
    "\n",
    "In the worst case, $\\mathsf{clip}(f(x), b) = b$, and $\\mathsf{clip}(f(x'),b) = 0$, so the sensitivity of the clipped result is exactly $b$ (the value of the clipping parameter).\n",
    "\n",
    "We can use the same trick to bound the $L2$ sensitivity of our gradient function. \n",
    "\n",
    "We'll need to define a function which \"clips\" a vector so that it has $L2$ norm within a desired range. \n",
    "\n",
    "We can accomplish this by *scaling* the vector: if we divide the vector elementwise by its $L2$ norm, then the resulting vector will have an $L2$ norm of 1. If we want to target a particular clipping parameter $b$, we can multiply the scaled vector by $b$ to scale it back up to have $L2$ norm $b$. We want to avoid modifying vectors that already have $L2$ norm below $b$; in that case, we just return the original vector. We can use `np.linalg.norm` with the parameter `ord=2` to calculate the $L2$ norm of a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2_clip(v, b):\n",
    "    norm = np.linalg.norm(v, ord=2)\n",
    "    if norm > b:\n",
    "        return b * (v / norm)\n",
    "    else:\n",
    "        return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to analyze the sensitivity of the clipped gradient. We denote the gradient as $\\nabla(\\theta; X, y)$ (corresponding to `gradient` in our Python code):\n",
    "\n",
    "\\begin{align}\n",
    "\\lVert \\mathsf{L2\\_clip}( \\nabla (\\theta; X, y), b) - \\mathsf{L2\\_clip}( \\nabla (\\theta; X', y), 0) \\rVert_2\n",
    "\\end{align}\n",
    "\n",
    "In the worst case, $\\mathsf{L2\\_clip}( \\nabla (\\theta; X, y), b)$ has $L2$ norm of $b$, and $\\mathsf{L2\\_clip}( \\nabla (\\theta; X', y))$ is all zeros - so that the $L2$ norm of the difference is equal to $b$. Thus, the $L2$ sensitivity of the clipped gradient is bounded by the clipping parameter $b$!\n",
    "\n",
    "Now we can proceed to compute the sum of clipped gradients, and add noise based on the $L2$ sensitivity $b$ that we've enforced by clipping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_sum(theta, X, y, b):\n",
    "    gradients = [L2_clip(gradient(theta, x_i, y_i), b) for x_i, y_i in zip(X,y)]\n",
    "\n",
    "    # sum query\n",
    "    # L2 sensitivity is b (by clipping performed above)\n",
    "    return np.sum(gradients, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to complete our noisy gradient descent algorithm. To compute the noisy average gradient, we need to:\n",
    "\n",
    "1. Add noise to the sum of the gradients based on its sensitivity $b$\n",
    "2. Compute a noisy count of the number of training examples (sensitivity 1)\n",
    "3. Divide the noisy sum from (1) by the noisy count from (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noisy_gradient_descent(iterations, epsilon, delta):\n",
    "    theta = np.zeros(X_train.shape[1])\n",
    "    sensitivity = 5.0\n",
    "\n",
    "    # Laplace Mech on Count\n",
    "    noisy_count = laplace_mech(X_train.shape[0], 1, epsilon)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        grad_sum        = gradient_sum(theta, X_train, y_train, sensitivity)\n",
    "        noisy_grad_sum  = gaussian_mech_vec(grad_sum, sensitivity, epsilon, delta)\n",
    "        noisy_avg_grad  = noisy_grad_sum / noisy_count\n",
    "        theta           = theta - noisy_avg_grad\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7799646174259177"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = noisy_gradient_descent(10, 0.1, 1e-5)\n",
    "accuracy(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each iteration of this algorithm satisfies $(\\epsilon, \\delta)$-differential privacy, and we perform one additional query to determine the noisy count which satisfies $\\epsilon$-differential privacy. \n",
    "\n",
    "If we perform $k$ iterations, then by **sequential composition**, the algorithm satisfies $(k\\epsilon + \\epsilon, k\\delta)$-differential privacy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity of the Gradient\n",
    "\n",
    "Our previous approach is very general, since it makes no assumptions about the behavior of the gradient. \n",
    "\n",
    "Sometimes, however, we *do* know something about the behavior of the gradient. \n",
    "\n",
    "In particular, a large class of useful gradient functions (including the gradient of the logistic loss, which we're using here) are *Lipschitz continuous* - meaning they have bounded global sensitivity. \n",
    "\n",
    "Formally, it is possible to show that:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{If}\\; \\lVert x_i \\rVert_2 \\leq b\\; \\text{then}\\; \\lVert \\nabla(\\theta; x_i, y_i) \\rVert_2 \\leq b\n",
    "\\end{align}\n",
    "\n",
    "This fact allows us to clip the values of the *training examples* (i.e. the *inputs* to the gradient function), instead of the *output* of the gradient function, and obtain a bound on the $L2$ sensitivity of the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clipping Training Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Clipping the training examples instead of the gradients has two advantages. \n",
    "\n",
    "1. it's often easier to estimate the scale of the training data (and thus to pick a good clipping parameter) than it is to estimate the scale of the gradients you'll compute during training. \n",
    "\n",
    "2. It's computationally more efficient: we can clip the training examples *once*, and re-use the clipped training data every time we train a model; with gradient clipping, we need to clip each gradient during training. \n",
    "\n",
    "Furthermore, we're no longer forced to compute per-example gradients so that we can clip them; instead, we can compute all of the gradients at once, which can be done very efficiently \n",
    "    - this is a commonly used trick in machine learning.\n",
    "\n",
    "> **Note**:\n",
    "> Many useful loss functions - in particular, those derived from neural networks in deep learning - do *not* have bounded global sensitivity. For these loss functions, we're forced to use gradient clipping.\n",
    "\n",
    "We can clip the training examples instead of the gradients with a couple of simple modifications to our algorithm. \n",
    "\n",
    "First, we clip the training examples using `L2_clip` before we start training. \n",
    "\n",
    "Second, we simply delete the code for clipping the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_sum(theta, X, y, b):\n",
    "    gradients = [gradient(theta, x_i, y_i) for x_i, y_i in zip(X,y)]\n",
    "\n",
    "    # sum query\n",
    "    # L2 sensitivity is b (by sensitivity of the gradient)\n",
    "    return np.sum(gradients, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noisy_gradient_descent(iterations, epsilon, delta):\n",
    "    theta = np.zeros(X_train.shape[1])\n",
    "    sensitivity = 5.0\n",
    "\n",
    "    noisy_count = laplace_mech(X_train.shape[0], 1, epsilon)\n",
    "    clipped_X = [L2_clip(x_i, sensitivity) for x_i in X_train]\n",
    "\n",
    "    for i in range(iterations):\n",
    "        grad_sum        = gradient_sum(theta, clipped_X, y_train, sensitivity)\n",
    "        noisy_grad_sum  = gaussian_mech_vec(grad_sum, sensitivity, epsilon, delta)\n",
    "        noisy_avg_grad  = noisy_grad_sum / noisy_count\n",
    "        theta           = theta - noisy_avg_grad\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7854931446262715"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = noisy_gradient_descent(10, 0.1, 1e-5)\n",
    "accuracy(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many improvements to this algorithm are possible, which can improve privacy cost and accuracy. Many are drawn from the machine learning literature. Some examples include:\n",
    "\n",
    "- Bounding the *total* privacy cost by $\\epsilon$ by calculating a per-iteration $\\epsilon_i$ as part of the algorithm.\n",
    "- Better composition for large numbers of iterations via advanced composition, RDP, or zCDP.\n",
    "- Minibatching: calculating the gradient for each iteration using a small chunk of the training data, rather than the whole training set (this reduces the computation needed to calculate the gradient).\n",
    "- Parallel composition in conjunction with minibatching.\n",
    "- Random sampling of batches in conjunction with minibatching.\n",
    "- Other hyperparameters, like a learning rate $\\eta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of Noise on Training\n",
    "\n",
    "So far, we've seen that the number of iterations has a big effect on the accuracy of the model we get, since more iterations can get you closer to the minimum of the loss. Since our differentially private algorithm adds noise to the gradient, this can also affect accuracy - the noise can cause our algorithm to move in *the wrong direction* during training, and actually make the model *worse*.\n",
    "\n",
    "It's reasonable to expect that smaller values of $\\epsilon$ will result in less accurate models (since this has been the trend in every differentially private algorithm we have seen so far). This is true, but there's also a slightly more subtle tradeoff which occurs because of the composition we need to consider when we perform many iterations of the algorithm: more iterations means a larger privacy cost. In the standard gradient descent algorithm, more iterations generally result in a better model. In our differentially private version, more iterations can make the model *worse*, since we have to use a smaller $\\epsilon$ for each iteration, and so the scale of the noise goes up. In differentially private machine learning, it's important (and sometimes, very challenging) to strike the right balance between the number of iterations used and the scale of the noise added.\n",
    "\n",
    "Let's do a small experiment to see how the setting of $\\epsilon$ effects the accuracy of our model. We'll train a model for several values of $\\epsilon$, using 20 iterations each time, and graph the accuracy of each model against the $\\epsilon$ value used in training it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "delta = 1e-5\n",
    "\n",
    "epsilons = [0.001, 0.003, 0.005, 0.008, 0.01, 0.03, 0.05, 0.08, 0.1]\n",
    "thetas   = [noisy_gradient_descent(10, epsilon, delta) for epsilon in epsilons]\n",
    "accs     = [accuracy(theta) for theta in thetas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjUAAAGoCAYAAAC+BRSYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABCw0lEQVR4nO3de3xU9Z3/8XdmciETQrgEgiAajAlsWSghQS4uoqQoroDgIqh4RbxFLKwtpayLChR0udQahYoKIl2wKNYF1gv2x1phLTYioOnWknBHUykJl0Aml7mc3x/JDBmSQAJnzmSG1/PxyCOZc86cfM8HZN5+L+dEGYZhCAAAIMzZQt0AAAAAMxBqAABARCDUAACAiECoAQAAEYFQAwAAIgKhBgAARARCDQAAiAiEGgAAEBGiQ90AK7ndbp08eVJxcXGy2chzAACEA6/Xq6qqKiUlJSk6uvHockmFmpMnT+rAgQOhbgYAALgAqamp6tChQ6P7L6lQExcXJ6mmKPHx8Rd8Ho/Ho8LCQmVkZMhut5vVPDSAWluHWluHWluHWlsnmLWuqKjQgQMH/J/jjbmkQo1vyCk+Pl4Oh+OCz+PxeCRJDoeD/0iCjFpbh1pbh1pbh1pbx4pan2/qCBNLAABARCDUAACAiECoAQAAEYFQAwAAIgKhBgAARARCDQAAiAiEGgAAEBEINQAAICIQagAAQEQg1AAAgIhAqAEAABGBUAMAACLCJfVASwAA0DyGYcjjNVTt8ara7fV/d3mMmte12yqrXaqq8oa0rYQaAABaAI/XOCs0BIaIao9XrrP2VwWEC0/Nz/XCx1nn8W3zeOVyG6o66zhXneOqa48zjKZdQ7tWNn1xTRMPDoKQhJrS0lLNmjVL+fn5stvtGj16tGbMmKHo6MDmTJ48WV9++WXANqfTqQkTJmjOnDmqrKzU/PnztXnzZlVXV+sHP/iBZs6cqZ49e1p5OQCAMOH19TicNyDU3VYnVDQWENxeVXuMeuHDd44GQ0PAeWt6Q8JBVJQUa7fVfEXXfMXYbYq1R+kf2hmKiooKWdtCEmqmTZumlJQUbd26VSUlJXrssce0cuVKTZ48OeC4119/PeD1unXr9PLLL2vKlCmSpJdeekkHDhzQ+++/L4fDocWLF2vKlCn6f//v/1l2LQBwqTEMQy6PIbe35v/0XV6v3B5DrtoPcVftz27vmW1n9vv21fm5zj637xxe389nHeM15HLXf3917Tn9P9eeq9rjVWWVS57/+r2/TeEiMDRE1QkPNsX5fq7zPTba5g8bMdFRirXba7dF1Tsupu45zj5PA783ts4+uy2qweDi8Xi0a9cu6wtVh+Wh5uDBg8rPz9eWLVsUHx+vbt26KTc3VwsXLqwXaurat2+f5s6dq+XLl6tTp06SpL1798owDBm1/WI2m03x8fGWXAcAXCjfHIVKl0de15kPc/+HsderandtaPB/2NcJCN6zAsI5gkRD5zyzvaH31QkWjZwznILBGZ4Gt8bYo2pDQOAH99kf8mf2RzUeAuocFxNtU9xZx9ULCGeFirrnjbE3HBxwbpaHmqKiIrVt21YpKSn+bWlpaSouLlZZWZnatGnT4Ptmz56tMWPGKDs7279t0qRJeuKJJzRw4EDZ7Xa1a9dOq1atOm8bPB6PPJ6G/4I3he+9F3MONA21tk441trrNVTh8tT54D3Xh/yZngW3t2aowH12D8DZvQZn/Z9/gx/+Z73P7anpuTjzs69noU7bvEbNHIV3fx/qEpom2halGLtN0faa7zG2qDM/223199ujFG2r+e5/XXtcbO1x0bXbY2w1PQ/RtprtsbXH+fcH/I4zvyfaZpMtytCBvXv0jz/oqVYx0WeFkZYaHAx5wzA4BvPfkKae0/JQU15eXq83xffa6XQ2GGq2b9+ur776SosWLQrY7vF4dNNNN+nxxx9XQkKCFixYoNzcXG3YsEFxcXGNtqGwsNCEK5EKCgpMOQ/Oj1pbx+paewxDzmpDp11elVcbKnd5dbq65mffttPVXpUH7DdUXu2V02UotGstzGOTFG2T7LYo2W01ISE6qua1b3uMTbJH1dlf+90eVWd/7euA/TYpOiqqwfNH2wLPZ486631nna+mPb62ndl/8eHAUGO9Ked9m7v26yze2q/L20TrxLd7Lqp1aLpQ/ntteahxOByqqKgI2OZ7nZCQ0OB71q5dq5tvvlkdO3b0b3O5XJo6dapeffVVf6/PrFmz1L9/f3322WcaNmxYo23IyMiQw+G44GvweDwqKChQ7969ZbfbL/g8OD9qbZ2LqbXL49XJCpfKKlw6WeFWWaWr9rVbJytcOlkZuK/m55rXp6sa+DS6QP7/S7cF/t//2b0Cgb0JtjPbGu1N8J3rzPlrehMCzxVTp4ch1n5Wz0Kd99pkqKjwr+rzj73UKiZa0bXzFGA+/g2xTjBr7XQ6m9QhYXmoSU9P14kTJ1RSUqLk5GRJNXNjOnfurMTExHrHu91ubd68WUuWLAnY7nQ6dfLkSVVXV/u32e12RUVFKSYm5pxtsNvtphTcrPPg/Kh18FW5PDpe4dG+0gqdrvLUhJJKl046a8LHSX8QqQklvuByssIlZ/XFdzc7Yu1Kio9RUnyM2tR+T4qPUZtWvp+jleQ4s923r3Wr6HNOXmyJPB6Pvo+1KckRx99ri/BviHWCUeumns/yUJOamqqsrCzNnz9fc+bM0fHjx7V06VKNGzeuweN3796tqqoq9evXL2B7UlKSsrKytGjRIv36179W69at9atf/Urt2rVTVlaWFZcCtCiGUTO/xB8+nLUBpNIdEEbODie+n6vcvoGcoxfchsS46IBA4v9yxKhNq+h6gcX3ldgqRrHR3OAcwMUJyZLuvLw8zZkzRzk5ObLZbBozZoxyc3MlSZmZmZo9e7ZGjx4tSTp8+LCSkpIanCOTl5enBQsWaPTo0XK73frhD3+o5cuXX9TQEhBKhmHodFX9XpGAoZwGgomv18TlubjJhVFSg6Gjjf97dP3AUieYMIQCIJRCEmqSk5OVl5fX4L6dO3cGvB4xYoRGjBjR6HkWLFhgevuAi2EYRuMBpLLxnhLf64td9BBti6oTQuoGj+izhnMCj0mMs6nomz+rX2Ym3fQAwhKPSQAuQJXbo2+PV+hQqVOHjjl1sPb7oWPlOnTMqUrXxa3JiY221QaQhntG2jQyjJMUHyNHrP2C5pZ4PB7ZwmROCgA0hFADNMAwDB13umoDS7kOBwQXp74vqzzvs1DiY+z1hmzODiL+XpOzJsC2iqGnBACai1CDS5bL41XxiQp/T8vhY86An0+dZ6mxI9auK9o7dGUHh65o79AVHRJqvrd3qEvbVoqLJpgAgJUINYhoZZUu/xBR3cBy8Fi5ik9UnvcBcp3btKoNLA5/YPH93CEhNmyWEAPApYBQg7Dm8Rr6vqyyNriUnxVcnDrhdJ3z/XHRNn9Y6Van1+XKDg5d3s7BMBAAhBFCDVq8SrdXu78/pW9PVPp7XA4dc+pQqVPfHq9Qtefck3KTW8fWBJb2gcNEV3ZwqGPrONlYhgwAEYFQg5AzDENHT1WdtYrI6Z+kW3K6WtLfG31/jD1Kl7dznBVczvS+tI7jrzkAXAr41x6WqHJ7dPhYRe0qonIdOlbhHy5qyhLopPgYXdnhrOBSG14uS4rnpm8AAEINzHGxS6BtUVKXtvH+YaGa8JKgrm3jdPK7vfqna/pxQzgAwDkRatBkDS2B9gWXpiyBToi1185nideVHRICel26totXjL3+s388Ho92HeWZQACA8yPUIIApS6B9K4hqh4d84aU9S6ABAEFEqLnEmLUE2jdEVPfmcyyBBgCEEqHmElBW6dIvPy7UlsKjTV4CfUUDy5+vaM8SaABAy0WoiXD/89cj+rff/Vnfl1X6t/mWQF9Rp6fFd+O5bu0cSmAJNAAgDPHpFaFOOKs1Z+Nf9Lud30mSUjs4NPOf/0G9urRhCTQAICIRaiLQR3/+Xv/+X39Wyekq2aKkB/+pu54c3kPxscx3AQBELkJNBCk9XaVnNvyf/vvrv0mS0jomaOHtP1S/K9qFuGUAAAQfoSYCGIah//76b3pmw//pWHm17LYoPXLdVfpxTjqrkQAAlwxCTQT4+bsFWrv9sCSpZ+dELRz3Q/W+PCnErQIAwFqEmjB3+JhTa7cfli1KemJYuh6/4WrFRnMHXgDApYdQE+YKj5ySJGWkJOpfh2eEuDUAAIQO/0sf5gqPnJYkpackhrglAACEFqEmzBX5emo6tQ5xSwAACC1CTZgr/HtNqKGnBgBwqSPUhDGv19Cev9cMP2Wk0FMDALi0EWrC2OHjTlW6vIqNtunKDgmhbg4AACFFqAljRbWThNM6tuZZTgCASx6hJoz559MwSRgAAEJNOPP11DCfBgAAQk1Y8914j5VPAAAQasKWJ2DlE6EGAABCTZg6fMypKrdXcdE2XdHeEermAAAQcoSaMFX0d1Y+AQBQF6EmTJ2ZT8MkYQAAJEJN2Cqq83RuAABAqAlb/qdzc48aAAAkEWrCksdraO9RVj4BAFBXSEJNaWmpcnNzlZ2drQEDBmjevHlyu931jps8ebIyMzMDvnr06KGnn37af8yaNWs0fPhwZWZmatSoUfrkk0+svJSQOFRn5VM3Vj4BACApRKFm2rRpcjgc2rp1q9atW6dt27Zp5cqV9Y57/fXXtXPnTv/XU089pcsuu0xTpkyRJL333ntasmSJFi9erB07duiRRx7RE088oSNHjlh8Rdbyzae5uhMrnwAA8Im2+hcePHhQ+fn52rJli+Lj49WtWzfl5uZq4cKFmjx5cqPv27dvn+bOnavly5erU6dOkqQVK1Zo6tSp6tOnjyRp5MiR6t69u1q3jux5JkXcdA8AgHosDzVFRUVq27atUlJS/NvS0tJUXFyssrIytWnTpsH3zZ49W2PGjFF2drYkqaKiQkVFRbLZbJo4caL27Nmj7t2766c//akSEhLO2QaPxyOPx3PB1+B778Wc42Ls/r5MkpTW0RGyNlgl1LW+lFBr61Br61Br6wSz1k09p+Whpry8XPHx8QHbfK+dTmeDoWb79u366quvtGjRIv+2srIyGYahFStW6MUXX9SVV16pt99+Ww899JA2btyoyy+/vNE2FBYWmnItBQUFppynub4+UCJJii4/ql27ToWkDVYLVa0vRdTaOtTaOtTaOqGsteWhxuFwqKKiImCb73VjPSxr167VzTffrI4dO/q3xcTESJIeeOABpaenS5LuvvtuvfXWW/r00081ceLERtuQkZEhh+PCJ9h6PB4VFBSod+/estvtF3yeC/rdXkPF7/1eknTjgD66skNkTxQOZa0vNdTaOtTaOtTaOsGstdPpbFKHhOWhJj09XSdOnFBJSYmSk5MlSXv37lXnzp2VmFh/jojb7dbmzZu1ZMmSgO3t27dXhw4dVF1dHbC9KV1UdrvdlIKbdZ7mOHjstKrdXrWKsSk1ubVsl8hE4VDU+lJFra1Dra1Dra0TjFo39XyWr35KTU1VVlaW5s+fr9OnT+vw4cNaunSpxo0b1+Dxu3fvVlVVlfr161dv3x133KElS5bom2++kdvt1qpVq3TkyBH96Ec/CvZlhIzvpntXd7p0Ag0AAE0RkiXdeXl5crvdysnJ0fjx4zVkyBDl5uZKkjIzM7Vhwwb/sYcPH1ZSUpLi4uLqnWfKlCmaPHmypk2bpv79+2v9+vV67bXXAiYhR5o9f699PEInVj4BAFCX5cNPkpScnKy8vLwG9+3cuTPg9YgRIzRixIgGj7XZbJo0aZImTZpkehtbKn9PDQ+yBAAgAI9JCDO+p3PTUwMAQCBCTRhxe7zad7RcEjfeAwDgbISaMHLwmFPVHq/iY+y6vF38+d8AAMAlhFATRuo+84mVTwAABCLUhJGi2knC6UwSBgCgHkJNGCnkQZYAADSKUBNGfMNP6Z3oqQEA4GyEmjDByicAAM6NUBMmDpSeWfnUtS0rnwAAOBuhJkz4h55SWPkEAEBDCDVhoqh2knA6dxIGAKBBhJow4X88Asu5AQBoEKEmTHCPGgAAzo1QEwZcHq/2lTD8BADAuRBqwsDB0nK5PIYcsax8AgCgMYSaMFDoG3rimU8AADSKUBMGzsynYegJAIDGEGrCQOHfWfkEAMD5EGrCwJlnPtFTAwBAYwg1LZzL49X+kppnPrGcGwCAxhFqWrgDJTUrnxJY+QQAwDkRalo438qnq1MSFRXFyicAABpDqGnh/I9H6MTQEwAA50KoaeH21D7IMoPl3AAAnBOhpoXz9dQwSRgAgHMj1LRg1e66K5/oqQEA4FwINS3YgdJyub2GWsdFq0tSq1A3BwCAFo1Q04L5hp6u7tSalU8AAJwHoaYF8y3n5vEIAACcH6GmBdvjf+YT82kAADgfQk0LVsjTuQEAaDJCTQtV7fbqgG/lEzfeAwDgvAg1LdT+kpqVT4lx0bqMlU8AAJwXoaaF8q98SmHlEwAATUGoaaGK/M98Yj4NAABNQahpoYr+7pskzHwaAACaglDTQvmfzs3KJwAAmiQkoaa0tFS5ubnKzs7WgAEDNG/ePLnd7nrHTZ48WZmZmQFfPXr00NNPP13v2HfeeUc9evSwovlBV+X26ECpUxI9NQAANFV0KH7ptGnTlJKSoq1bt6qkpESPPfaYVq5cqcmTJwcc9/rrrwe8XrdunV5++WVNmTIlYHtRUZHmz58f9HZbZX9JuTy1K586t2HlEwAATWF5T83BgweVn5+v6dOnKz4+Xt26dVNubq5Wr159zvft27dPc+fO1aJFi9SpUyf/9oqKCj355JO69957g910y5y56R4rnwAAaCrLQ01RUZHatm2rlJQU/7a0tDQVFxerrKys0ffNnj1bY8aMUXZ2dsD2OXPm6Prrr9fgwYOD1marFTGfBgCAZrN8+Km8vFzx8fEB23yvnU6n2rRpU+8927dv11dffaVFixYFbF+/fr327t2ruXPn6ssvv2xyGzwejzwezwW0/sz76343m/8eNR0TgvY7wkWwa40zqLV1qLV1qLV1glnrpp7T8lDjcDhUUVERsM33OiEhocH3rF27VjfffLM6duzo37Zv3z4tXrxYq1evVnR08y6jsLCwma1uWEFBgSnnqXfeQyWSJNvpI9q160RQfke4CVatUR+1tg61tg61tk4oa215qElPT9eJEydUUlKi5ORkSdLevXvVuXNnJSbWH25xu93avHmzlixZErB906ZNKisr09ixYyWdSXHZ2dl65plnNGrUqEbbkJGRIYfDccHX4PF4VFBQoN69e8tut1/weRpS5fbqyLu/lySNGPRDpVziE4WDWWsEotbWodbWodbWCWatnU5nkzokLA81qampysrK0vz58zVnzhwdP35cS5cu1bhx4xo8fvfu3aqqqlK/fv0Ctj/22GN67LHH/K//9Kc/6d5779X27dvP2wa73W5Kwc06T10H/1678qlVtC5r62CicK1g1BoNo9bWodbWodbWCUatm3q+kNynJi8vT263Wzk5ORo/fryGDBmi3NxcSVJmZqY2bNjgP/bw4cNKSkpSXFxcKJpqubo33SPQAADQdCG5T01ycrLy8vIa3Ldz586A1yNGjNCIESPOe84BAwZo9+7dprQvlIpql3NncNM9AACahccktDC+npp0HmQJAECzEGpamD1/9/XUEGoAAGgOQk0LUuny6EBpuSSGnwAAaC5CTQuy72i5vIbUplW0OiZeGhOjAQAwC6GmBSn6OyufAAC4UISaFsQ/SZj5NAAANBuhpgX5v+KaB3oynwYAgOYj1LQQB0rKtaXwqCRpcFpyiFsDAED4IdS0EMu27JPXkG7o0VE9OjP8BABAcxFqWoAjZZV698tvJUm5N1wd4tYAABCeCDUtwPL/3a9qj1f9U9upf2r7UDcHAICwRKgJsZNOl1Z/flCS9Nj1aSFuDQAA4YtQE2Krth1QebVHPTsn6oYenULdHAAAwhahJoQqqj16448HJNX00nDDPQAALhyhJoR++8UhHSuv1hXtHbql92Whbg4AAGGNUBMi1W6vXtuyT5L0yNCrFG3njwIAgIvBJ2mIrN/1nYpPVqpjYpz+pd/loW4OAABhj1ATAl6voVc+3StJevCfuqtVjD3ELQIAIPwRakLg478c0d6j5WrTKloTB1wR6uYAABARCDUWMwxDv/7DHknSvYNSldgqJsQtAgAgMhBqLPbHvaX66tuTahVj0wPXpoa6OQAARAxCjcWW1vbSTMjupg6t40LcGgAAIgehxkJfHT6hz/aUKtoWpYeuuyrUzQEAIKIQaiz06z/UrHga3beLLm/nCHFrAACILIQai+z5+2lt+sv3kqTHhvLgSgAAzEaoscirW/bKMKQbf5Ci9JTEUDcHAICIQ6ixyJ/2H5Mk3T3wyhC3BACAyESosUh5lUeS1DGRFU8AAAQDocYiFdVuSZIjlkciAAAQDIQaCxiGIaerpqfGERsd4tYAABCZCDUWqHR5ZRg1P9NTAwBAcBBqLOCsHXqSpHieyA0AQFAQaizgrK4ZeoqPsctmiwpxawAAiEyEGgv4Qg1DTwAABA+hxgK+4ad4Qg0AAEFDqLGAr6cmgZVPAAAETbNCzc9//nN98cUXwWpLxPLPqaGnBgCAoGlW14HD4dATTzyhxMREjR07Vrfddps6d+7c7F9aWlqqWbNmKT8/X3a7XaNHj9aMGTMUHR3YnMmTJ+vLL78M2OZ0OjVhwgTNmTNHVVVVWrRokTZt2qTy8nJdddVV+slPfqKBAwc2u03B5OTGewAABF2zemqefvppbd26VdOnT1dBQYFuvPFGPfjgg/rggw9UXV3d5PNMmzZNDodDW7du1bp167Rt2zatXLmy3nGvv/66du7c6f966qmndNlll2nKlCmSpEWLFmnHjh1au3at8vPzdfvtt+vRRx9VcXFxcy4r6M5MFGb4CQCAYGn2nJqYmBjdeOON+vWvf61Vq1bp+PHjevLJJzVkyBD9x3/8h06dOnXO9x88eFD5+fmaPn264uPj1a1bN+Xm5mr16tXnfN++ffs0d+5cLVq0SJ06dZIkVVVV6cc//rEuu+wy2e12jR8/XrGxsfq///u/5l5WULH6CQCA4Gt2qDl69KjeeOMNjRkzRvfcc4+6dOmipUuX6s0339T+/fv12GOPnfP9RUVFatu2rVJSUvzb0tLSVFxcrLKyskbfN3v2bI0ZM0bZ2dn+bXPmzNHQoUP9r7dt26ZTp06pZ8+ezb2soOK5TwAABF+zxkMefPBBff7557rqqqt022236dZbb1X79u39+5988klNmDDhnOcoLy9XfHx8wDbfa6fTqTZt2tR7z/bt2/XVV19p0aJFjZ53165dmjZtmqZMmaJu3bqdsw0ej0cej+ecx5zv/XW/n8/pSpckKT7GdlG/91LU3FrjwlFr61Br61Br6wSz1k09Z7NCzeWXX6633npLffr0aXB/165dtW7dunOew+FwqKKiImCb73VCQkKD71m7dq1uvvlmdezYscH977zzjubPn68f//jHeuCBB853GSosLDzvMU1RUFDQpOMOFdf0QJUdL9GuXbtM+d2XmqbWGhePWluHWluHWlsnlLVuVqh56qmntGTJErVr107dunXTm2++qWPHjmnq1Kmy2WxKSEhQWlraOc+Rnp6uEydOqKSkRMnJyZKkvXv3qnPnzkpMTKx3vNvt1ubNm7VkyZJ6+zwej2bPnq2PP/5YS5Ys0eDBg5t0HRkZGXI4HE06tiEej0cFBQXq3bu37PbzDykl7CuQ5FT3bl3Ut++564NAza01Lhy1tg61tg61tk4wa+10OpvUIdGsUPP8889r165d/iGmXr166fnnn5fL5dLPfvazJp0jNTVVWVlZmj9/vubMmaPjx49r6dKlGjduXIPH7969W1VVVerXr1+9fc8995y2bNmid999V127dm3yddjtdlMK3tTzOF1eSVLruBj+o7pAZv2Z4fyotXWotXWotXWCUeumnq9ZE4U3bdqk119/XV26dJEkZWdn65VXXtGGDRua1bi8vDy53W7l5ORo/PjxGjJkiHJzcyVJmZmZAec7fPiwkpKSFBcXF3COY8eOafXq1SopKdHIkSOVmZnp/2pue4KtgiXdAAAEXbM+ZauqquoN27Ru3Vput7tZvzQ5OVl5eXkN7tu5c2fA6xEjRmjEiBH1jmvfvr2++eabZv3eUOHZTwAABF+zemqys7P13HPP+W+0V1VVpQULFjQ4NIQz/M9+iiPUAAAQLM2eKDx58mT169dP7dq10/Hjx9W9e3e98sorwWpfRPA/+ymG4ScAAIKlWZ+y3bp10wcffKAvv/xSJSUl6ty5s/r06VPvmU0IVMEdhQEACLpmp5Hq6mpdccUVuvzyyyVJ3333nQoLCzV8+HDTGxcpymvn1DD8BABA8DQr1Lz77ruaO3euqqqqArZ36NCBUHMO/uEnVj8BABA0zfqUfeWVVzRt2jQlJCToiy++0H333aeFCxfq2muvDVb7wp7b41W1u+Y+NY4YemoAAAiWZq1+Onr0qO677z4NGjRIhw4dUq9evTR//ny98847wWpf2HO6zjyvwsHwEwAAQdOsUNOhQwe5XC5ddtll2r9/vySpS5cuKi0tDUrjIoFvkrDdFqVYe7Mfig4AAJqoWZ+yffr00dNPP63Kykqlpqbqrbfe0nvvvae2bdsGqXnhr7yqZpKwI8auqKioELcGAIDI1aw5NTNnztS///u/q7y8XNOnT9ejjz6qyspKPffcc8FqX9jzTRJm6AkAgOBqVqj54osv9NJLLykuLk6dOnXS559/LpfLpfj4+GC1L+xVuHjuEwAAVmjW8NPs2bNls515S3R0NIHmPHzDT/GsfAIAIKiaFWp69+6tDz74IFhtiUgVPPcJAABLNGtM5MSJE5oxY4ZmzZql5OTkgImvmzdvNr1xkYAb7wEAYI1mfdLefffdwWpHxHJWn1n9BAAAgqdZoWbs2LHBakfEYvUTAADWaFaoueeeexq918qqVatMaVCkcfKEbgAALNGsUDNgwICA18ePH9dHH32kCRMmmNqoSOIffmJODQAAQdWsT9opU6bU23bbbbdpwYIFpjUo0tBTAwCANS76YUS9evXSn//8ZzPaEpEqCDUAAFiiWT01xcXFAa9dLpfef/99XXbZZaY2KpKU1w4/saQbAIDgatYn7bBhwwImChuGoaSkJP3iF78wvWGRwjf8lEBPDQAAQdWsUHP2Dfbsdrs6dOigmJgYUxsVSRh+AgDAGs2aU9OpUye9/fbb8nq96tq1qzZt2qQlS5bI6/UGq31hr5w7CgMAYIlmhZr58+dry5Ytsttreh169eql//3f/9WiRYuC0rhIUFE7p4bhJwAAgqtZoebjjz/W8uXL1aVLF0lSdna2XnnlFW3YsCEojYsEZ579RKgBACCYmhVqqqqq5HA4Ara1bt1abrfb1EZFkjMThRl+AgAgmJoVarKzs/Xcc8+purpaUk3IWbBggfr16xeUxoU7wzDq3FGYnhoAAIKpWd0HTz31lB588EH169dP7dq10/Hjx9W9e3e98sorwWpfWKtye+U1an5m+AkAgOBqVqjp1q2bPvzwQ+3YsUNHjx5V586d1adPH0VHM7TSEN/Qk8SznwAACLZmDT+VlZXpZz/7mdq3b69//ud/1tatWzVz5kyVl5cHq31hzTf0FBdtk93W8NPNAQCAOZoVap599lmdPHlSbdu2lSSNHDlSp06d0vz584PRtrDHjfcAALBOs8ZE/vjHP2rz5s1KSEiQJKWlpWnRokUaPnx4UBoX7sr9oYahJwAAgq1ZPTVer1cejydgm2EY/pvxIRArnwAAsE6zQs11112nGTNm6NChQ3K5XDp06JBmzpypa6+9NljtC2sMPwEAYJ1mhZp/+7d/0+nTp3XjjTeqT58+uummm1RRUaEZM2YEq31hjeEnAACs06xP2/bt2+s3v/mNiouLdfToUXk8Hv3Xf/2Xhg0bpl27dgWpieGrguEnAAAsc0FdCMXFxVq+fLk+/fRTpaena/r06c16f2lpqWbNmqX8/HzZ7XaNHj1aM2bMqHe/m8mTJ+vLL78M2OZ0OjVhwgTNmTNHkvTaa6/pN7/5jcrKytS7d2/Nnj1bV1111YVclunKq3juEwAAVmlyqPF6vfroo4/0xhtvqKioSG63W8uWLdOQIUOa/UunTZumlJQUbd26VSUlJXrssce0cuVKTZ48OeC4119/PeD1unXr9PLLL2vKlCmSpPfee0+/+c1vtHz5cl1xxRV64YUX9OMf/1gbN25UVFTo7wtT4eK5TwAAWKVJc2refPNNDR8+XAsXLtTw4cP1hz/8Qa1bt1ZGRkazf+HBgweVn5+v6dOnKz4+Xt26dVNubq5Wr159zvft27dPc+fO1aJFi9SpUydJ0ttvv6277rpL6enpiouL009+8hMVFxfrT3/6U7PbFQy+1U/01AAAEHxNCjXPPfechg4dqk2bNunhhx/233zvQhQVFalt27ZKSUnxb0tLS1NxcbHKysoafd/s2bM1ZswYZWdn+7ft2bMnIFjFxMQoNTVVf/3rXy+4fWbyDT8xpwYAgOBr0rjIrFmztGbNGg0dOlTjx4/XXXfddcHDO+Xl5YqPjw/Y5nvtdDrVpk2beu/Zvn27vvrqKy1atOi852rVqpWcTuc52+DxeOrdb6c5fO893zmcVbU9NTG2i/p9l7Km1hoXj1pbh1pbh1pbJ5i1buo5mxRqJk6cqIkTJ2rbtm36z//8Tw0fPlwej0fbtm3TqFGjmnXzPYfDoYqKioBtvte+OxWfbe3atbr55pvVsWPHgO3x8fGqrKwM2FZZWdnoeXwKCwub3N5zKSgoOOf+7/5+QpJ07Oj32rXrlCm/81J1vlrDPNTaOtTaOtTaOqGsdbNmsA4aNEiDBg3Sd999pzVr1uj555/XggULNHr0aP385z9v0jnS09N14sQJlZSUKDk5WZK0d+9ede7cWYmJifWOd7vd2rx5s5YsWdLguYqKinTDDTdIklwulw4cOHDeuT4ZGRlyOBxNam9DPB6PCgoK1Lt373MGurivv5RUqfTUK9S3b7cL/n2XsqbWGhePWluHWluHWlsnmLV2Op1N6pC4oGU5Xbt21fTp0zV16lRt2LBBa9asafJ7U1NTlZWVpfnz52vOnDk6fvy4li5dqnHjxjV4/O7du1VVVaV+/frV2/cv//Iveumll3Tdddepe/fueuGFF5ScnBww76YhdrvdlIKf7zwVLq8kqXV8LP8xXSSz/sxwftTaOtTaOtTaOsGodVPP16w7Cp8tNjZW48aN0+9+97tmvS8vL09ut1s5OTkaP368hgwZotzcXElSZmamNmzY4D/28OHDSkpKUlxcXL3zjBs3Tvfff78ef/xxDRw4UH/5y1+0bNkyxcTEXMxlmcZZu6TbEcN/SAAABFtIbqCSnJysvLy8Bvft3Lkz4PWIESM0YsSIBo+NiorSpEmTNGnSJNPbaAbfRGFWPwEAEHwX1VODc3P6nv0Ux833AAAINkJNEPnuKExPDQAAwUeoCaJy/31qCDUAAAQboSZIPF5DVe6a1U8JDD8BABB0hJog8Q09SQw/AQBgBUJNkPhWPkVFSXHRlBkAgGDj0zZIfCufEmKjL/g5WQAAoOkINUHiCzXxDD0BAGAJQk2QOKu58R4AAFYi1ASJ/8Z7sax8AgDACoSaIDkTauipAQDACoSaIGH4CQAAaxFqgoSeGgAArEWoCZIK5tQAAGApQk2QlNcOP7GkGwAAaxBqgqTCf/M9Qg0AAFYg1ATJmZvvMfwEAIAVCDVBUs7qJwAALEWoCRKGnwAAsBahJkjKGX4CAMBShJogqagdfqKnBgAAaxBqgoSndAMAYC1CTZDwQEsAAKxFqAkSnv0EAIC1CDVBwrOfAACwFqEmCAzDYPgJAACLEWqCoNrjlcdrSJIccfTUAABgBUJNEPhuvCdJjhhCDQAAViDUBIHvxnuxdpui7ZQYAAAr8IkbBL4b7zH0BACAdQg1QeCfJMzQEwAAliHUBEF5FXcTBgDAaoSaIKhw1T73KY7l3AAAWIVQEwT+5z4x/AQAgGUINUHgrOJuwgAAWI1QEwT+5z4x/AQAgGUINUHgdLH6CQAAqxFqgoDhJwAArBeSUFNaWqrc3FxlZ2drwIABmjdvntxud4PH5ufn6/bbb1dmZqaGDh2qZcuW+fdVVlbq6aef1rXXXqv+/fvrvvvu01//+lerLqNR/vvUMPwEAIBlQhJqpk2bJofDoa1bt2rdunXatm2bVq5cWe+4vXv36uGHH9Zdd92lHTt2aNmyZVqxYoU++ugjSdJLL72kAwcO6P3339dnn32mnj17asqUKRZfTX2+Jd0MPwEAYB3LQ83BgweVn5+v6dOnKz4+Xt26dVNubq5Wr15d79g1a9YoJydHY8eOVVRUlHr27Knf/va3ysrKklQTegzDkGHUPBHbZrMpPj7e0utpCDffAwDAepaPjxQVFalt27ZKSUnxb0tLS1NxcbHKysrUpk0b//avv/5agwcP1pNPPqnPPvtM7du31/33368JEyZIkiZNmqQnnnhCAwcOlN1uV7t27bRq1arztsHj8cjj8Zz3uHO9v+73szmranpq4mNsF/V7cP5awzzU2jrU2jrU2jrBrHVTz2l5qCkvL6/Xm+J77XQ6A0LNyZMntWrVKr3wwgtasGCBdu7cqUceeURJSUkaMWKEPB6PbrrpJj3++ONKSEjQggULlJubqw0bNiguLq7RNhQWFppyLQUFBQ1uP3LshCTp6N++1a5dpab8rktdY7WG+ai1dai1dai1dUJZa8tDjcPhUEVFRcA23+uEhISA7bGxscrJydH1118vSerfv79uvfVWffjhh8rJydHUqVP16quv+nt9Zs2apf79++uzzz7TsGHDGm1DRkaGHA7HBV+Dx+NRQUGBevfuLbu9/hCT/fNtkqrV8+qr1PcHKfVPgCY7X61hHmptHWptHWptnWDW2ul0NqlDwvJQk56erhMnTqikpETJycmSaubGdO7cWYmJiQHHpqWlqbq6OmCbx+ORYRhyOp06efJkwH673a6oqCjFxMScsw12u92Ugjd2nopqryQpMT6W/4hMYtafGc6PWluHWluHWlsnGLVu6vksnyicmpqqrKwszZ8/X6dPn9bhw4e1dOlSjRs3rt6xd9xxhzZv3qz169fLMAx98cUX2rhxo2699VYlJSUpKytLixYtUmlpqaqqqrRw4UK1a9fOP5E4VJy1q5+YKAwAgHVCsqQ7Ly9PbrdbOTk5Gj9+vIYMGaLc3FxJUmZmpjZs2CBJGjRokJYuXapVq1YpKytLM2fO1IwZM5STk+M/T2pqqkaPHq3rrrtOe/fu1fLlyy9qaMkM3HwPAADrheTucMnJycrLy2tw386dOwNeDx06VEOHDm30PAsWLDC9fRfLd/O9hFhuvgcAgFV4TILJvF5DFS7uUwMAgNUINSbzBRqJ4ScAAKxEqDGZb+gpKkpqFU2oAQDAKoQakzmrfXcTtstmiwpxawAAuHQQakzmf0I3Q08AAFiKUGOyM6GGlU8AAFiJUGMy3/ATPTUAAFiLUGMyX08Ny7kBALAWocZkFdx4DwCAkCDUmKy8muc+AQAQCoQak1Ww+gkAgJAg1JiM1U8AAIQGocZk5ax+AgAgJAg1JjszUZhQAwCAlQg1JjuzpJvhJwAArESoMRk33wMAIDQINSbj2U8AAIQGocZkrH4CACA0CDUmY/gJAIDQINSYjOEnAABCg1BjsgqGnwAACAlCjcnKq3j2EwAAoUCoMVmFq/bme3GEGgAArESoMVG12yuXx5AkOWIYfgIAwEqEGhP55tNIDD8BAGA1Qo2JnK6a+TQx9ijFRlNaAACsxCevifzPfYqhlwYAAKsRakzkrGI5NwAAoUKoMZH/bsKsfAIAwHKEGhM5XdxNGACAUCHUmMg//MRybgAALEeoMRHDTwAAhA6hxkQ8zBIAgNAh1JjozJJuhp8AALAaocZEFbXDTzz3CQAA6xFqTFTu66lh+AkAAMsRakzkn1PD8BMAAJYLSagpLS1Vbm6usrOzNWDAAM2bN09ut7vBY/Pz83X77bcrMzNTQ4cO1bJlywL2r1mzRsOHD1dmZqZGjRqlTz75xIpLaBDDTwAAhE5IQs20adPkcDi0detWrVu3Ttu2bdPKlSvrHbd37149/PDDuuuuu7Rjxw4tW7ZMK1as0EcffSRJeu+997RkyRItXrxYO3bs0COPPKInnnhCR44csfiKajD8BABA6Fg+TnLw4EHl5+dry5Ytio+PV7du3ZSbm6uFCxdq8uTJAceuWbNGOTk5Gjt2rCSpZ8+e+u1vf6vWrVtLklasWKGpU6eqT58+kqSRI0eqe/fu/v1Wq2BJNwAAIWN5qCkqKlLbtm2VkpLi35aWlqbi4mKVlZWpTZs2/u1ff/21Bg8erCeffFKfffaZ2rdvr/vvv18TJkxQRUWFioqKZLPZNHHiRO3Zs0fdu3fXT3/6UyUkJJyzDR6PRx6P54Kvwffes89RXjv81CradlHnxxmN1Rrmo9bWodbWodbWCWatm3pOy0NNeXm54uPjA7b5XjudzoBQc/LkSa1atUovvPCCFixYoJ07d+qRRx5RUlKSMjMzZRiGVqxYoRdffFFXXnml3n77bT300EPauHGjLr/88kbbUFhYaMq1FBQUBLwuPXlakvS3wwe0q/pvpvwO1Di71ggeam0dam0dam2dUNba8lDjcDhUUVERsM33+uweltjYWOXk5Oj666+XJPXv31+33nqrPvzwQ11zzTWSpAceeEDp6emSpLvvvltvvfWWPv30U02cOLHRNmRkZMjhcFzwNXg8HhUUFKh3796y2+sMNW3+VJJb/9gzQ32vbHfB58cZjdYapqPW1qHW1qHW1glmrZ1OZ5M6JCwPNenp6Tpx4oRKSkqUnJwsqWZCcOfOnZWYmBhwbFpamqqrqwO2eTweGYah9u3bq0OHDg3uPx+73W5Kwc8+j7PaK0lKjI/lPx6TmfVnhvOj1tah1tah1tYJRq2bej7LVz+lpqYqKytL8+fP1+nTp3X48GEtXbpU48aNq3fsHXfcoc2bN2v9+vUyDENffPGFNm7cqFtvvdW/f8mSJfrmm2/kdru1atUqHTlyRD/60Y+svixJdR5oyURhAAAsF5Il3Xl5eXK73crJydH48eM1ZMgQ5ebmSpIyMzO1YcMGSdKgQYO0dOlSrVq1SllZWZo5c6ZmzJihnJwcSdKUKVM0efJkTZs2Tf3799f69ev12muvBUxCtophGKpwsaQbAIBQCcmtb5OTk5WXl9fgvp07dwa8Hjp0qIYOHdrgsTabTZMmTdKkSZNMb2NzVbq8MoyanxNiuaMwAABW4zEJJvEt55ak+Bh6agAAsBqhxiS+G++1irHJZosKcWsAALj0EGpM4nuYJUNPAACEBqHGJL7hJyYJAwAQGoQak/DcJwAAQotQYxKnP9Qw/AQAQCgQakzCjfcAAAgtQo1J6KkBACC0CDUmcTKnBgCAkCLUmMRZxfATAAChRKgxidPF8BMAAKFEqDEJS7oBAAgtQo1Jyqu4+R4AAKFEqDGJb/gpgVADAEBIEGpMUsGSbgAAQopQYxKGnwAACC1CjUkqfMNPcYQaAABCgVBjEn9PTQzDTwAAhAKhxiQs6QYAILQINSZxMvwEAEBIEWpM4qyqCTXxrH4CACAkCDUmcHu8qvZ4JUmOGHpqAAAIBUKNCXxDT5LkYPgJAICQINSYwDf0ZLdFKdZOSQEACAU+gU3grK5Zzu2IsSsqKirErQEA4NJEqDGB07ecm6EnAABChlBjAifPfQIAIOQINSbwDT/Fs/IJAICQIdSYwHc3YW68BwBA6BBqTFBezY33AAAINUKNCSrqrH4CAAChQagxAaufAAAIPUKNCcp5QjcAACFHqDGBf/iJOTUAAIQMocYETnpqAAAIOUKNCQg1AACEHqHGBP6b7zH8BABAyIQk1JSWlio3N1fZ2dkaMGCA5s2bJ7fb3eCx+fn5uv3225WZmamhQ4dq2bJlDR73zjvvqEePHsFsdqN8PTUJ9NQAABAyIQk106ZNk8Ph0NatW7Vu3Tpt27ZNK1eurHfc3r179fDDD+uuu+7Sjh07tGzZMq1YsUIfffRRwHFFRUWaP3++Ra2vj+EnAABCz/JQc/DgQeXn52v69OmKj49Xt27dlJubq9WrV9c7ds2aNcrJydHYsWMVFRWlnj176re//a2ysrL8x1RUVOjJJ5/Uvffea+VlBHByR2EAAELO8k/hoqIitW3bVikpKf5taWlpKi4uVllZmdq0aePf/vXXX2vw4MF68skn9dlnn6l9+/a6//77NWHCBP8xc+bM0fXXX6/BgwfrlVdeaVIbPB6PPB7PBV+D772+7745Na2ioy7qvKjv7FojeKi1dai1dai1dYJZ66ae0/JQU15ervj4+IBtvtdOpzMg1Jw8eVKrVq3SCy+8oAULFmjnzp165JFHlJSUpBEjRmj9+vXau3ev5s6dqy+//LLJbSgsLDTlWgoKCmraWV4pSTq8f4+ij8eYcm4E8tUawUetrUOtrUOtrRPKWlseahwOhyoqKgK2+V4nJCQEbI+NjVVOTo6uv/56SVL//v1166236sMPP1RGRoYWL16s1atXKzq6eZeRkZEhh8Nxwdfg8XhUUFCg3r17y263y/jvzZK86te7l67scOHnRX1n1xrBQ62tQ62tQ62tE8xaO53OJnVIWB5q0tPTdeLECZWUlCg5OVlSzYTgzp07KzExMeDYtLQ0VVdXB2zzeDwyDEObNm1SWVmZxo4d698uSdnZ2XrmmWc0atSoRttgt9tNKbjvPJP/qbsOHXOqe8fWioqKuujzoj6z/sxwftTaOtTaOtTaOsGodVPPZ/lE4dTUVGVlZWn+/Pk6ffq0Dh8+rKVLl2rcuHH1jr3jjju0efNmrV+/XoZh6IsvvtDGjRt166236rHHHtOuXbu0fft2bd++3T+fZvv27ecMNMHwRE66Ft7+QwINAAAhFJIl3Xl5eXK73crJydH48eM1ZMgQ5ebmSpIyMzO1YcMGSdKgQYO0dOlSrVq1SllZWZo5c6ZmzJihnJycUDQbAAC0YCFZg5ycnKy8vLwG9+3cuTPg9dChQzV06NDznnPAgAHavXu3Ke0DAADhh8ckAACAiECoAQAAEYFQAwAAIgKhBgAARARCDQAAiAiEGgAAEBEINQAAICIQagAAQEQg1AAAgIhAqAEAABGBUAMAACICoQYAAESEkDzQMlS8Xq8kqaKi4qLO4/F4JElOp1N2u/2i24XGUWvrUGvrUGvrUGvrBLPWvs9t3+d4Y6IMwzBM/c0tWGlpqQ4cOBDqZgAAgAuQmpqqDh06NLr/kgo1brdbJ0+eVFxcnGw2Rt4AAAgHXq9XVVVVSkpKUnR044NMl1SoAQAAkYvuCgAAEBEINbVKS0uVm5ur7OxsDRgwQPPmzZPb7W7w2E8//VSjRo1S3759dfPNN+uTTz4J2P/aa6/puuuuU9++fXXPPfdo3759VlxC2DCr1lVVVZo3b56uu+46ZWVl6fbbb9fnn39u1WWEBTP/Xvu888476tGjRzCbHZbMrPWaNWs0fPhwZWZmatSoUY3+WVyqzKp1ZWWlnn76aV177bXq37+/7rvvPv31r3+16jLCQnNq7bNp0ybl5OTU227JZ6MBwzAM4+677zZ+8pOfGE6n0zh06JBxyy23GK+99lq94/bv32/07t3b+P3vf2+4XC7j/fffN/r06WN8//33hmEYxu9+9ztjyJAhRmFhoVFZWWk899xzxi233GJ4vV6rL6nFMqvWv/jFL4zbbrvNKC4uNtxut7F27Vrjhz/8ofHdd99ZfUktllm19iksLDT69u1rZGRkWHUJYcPMf0MGDx5sfPXVV4bX6zU2btxo9OrVq96fxaXMrFovWLDAuOeee4zjx48bVVVVxvz5842cnByrL6dFa2qtDcMwqqurjVdffdX4wQ9+YNxwww0B+6z6bCTUGIZx4MABIyMjI+Afjffff9+4/vrr6x37y1/+0njggQcCtj344IPGiy++aBiGYdxxxx3Gr3/9a/++6upqIzMz09i2bVuQWh9ezKz1rFmzjD/84Q8B+/v37298/PHHQWh5+DGz1oZhGE6n0xg5cqTxy1/+klBzFjNrPXLkSGPt2rUB+//85z8bp0+fDkLLw4+ZtX7kkUeMu+++2zh27JhRVVVlPP/888bIkSODewFhpDm1NoyaAPTggw8aL7zwQr1QY9VnI8NPkoqKitS2bVulpKT4t6Wlpam4uFhlZWUBx+7Zs0cZGRkB266++mp/l+XZ+2NiYpSamkqXZi0zaz1nzhwNHTrUv2/btm06deqUevbsGcQrCB9m1lqqqff111+vwYMHB7fhYcisWldUVKioqEg2m00TJ07UgAEDdMcdd6iiokIJCQmWXEtLZ+bf60mTJqmwsFADBw5U3759tWHDBv3qV78K+jWEi+bUWpIWLlyo119/XVdccUW9fVZ9NhJqJJWXlys+Pj5gm++10+k877GtWrXyH3e+/Zc6M2td165duzRt2jRNmTJF3bp1M7nV4cnMWq9fv1579+7V1KlTg9ji8GVWrcvKymQYhlasWKFnn31WW7du1ciRI/XQQw/p22+/De5FhAkz/157PB7ddNNN2rJli/Lz85WTk6Pc3FxVVVUF8QrCR3NqLUmdO3du1rmC8dlIqJHkcDjq3WXY9/rs/zuKj49XZWVlwLbKykr/cefbf6kzs9Y+77zzjh544AE9+uijevzxx4PQ6vBkVq337dunxYsXa/Hixee8P8SlzKxax8TESJIeeOABpaenKzY2Vnfffbe6dOmiTz/9NIhXED7MqrXL5dLUqVN12223KSUlRa1bt9asWbN05MgRffbZZ8G9iDDRnFqfj1WfjYQaSenp6Tpx4oRKSkr82/bu3avOnTsrMTEx4NiMjAwVFRUFbNuzZ4/S09P956q73+Vy6cCBA/W6QC9VZtba4/Ho6aef1uLFi7VkyRI98MADwb+AMGJWrTdt2qSysjKNHTtW2dnZevTRRyVJ2dnZ2rhxY/AvJAyYVev27durQ4cOqq6uDtjvu/08zKu10+nUyZMnA2ptt9sVFRXlD5eXuubUuinnsuSz0dQZOmHszjvvNP71X//VOHXqlH+Gd15eXr3j9uzZY/Tu3dt4//33/bPpe/fubezbt88wDMN4++23jSFDhhjffPONf4b38OHDjerqaqsvqcUyq9Zz5841hg4danz77bdWX0LYMKvWdX3++edMFG6AWbV+8cUXjUGDBhl/+ctfDJfLZbz55ptG3759Wf1Uh1m1vvPOO43bb7/dKCkpMSorK43nn3/euOGGG4zy8nKrL6nFamqt63r33XfrTRS26rORUFPr6NGjxhNPPGFcc801xsCBA43nn3/ecLvdhmEYRt++fY3169f7j92yZYsxevRoo2/fvsYtt9wSsALH6/Uay5cvN4YNG2b07dvXuOeeexr8YLiUmVHr0tJSo2fPnkavXr2Mvn37BnzVff+lzqy/13URahpmVq09Ho+xfPly48YbbzT69u1r3HbbbcYXX3xh+fW0ZGbV+ujRo8b06dONwYMHG9dcc43x0EMP8e/1WZpTa5+GQo1Vn408JgEAAEQE5tQAAICIQKgBAAARgVADAAAiAqEGAABEBEINAACICIQaAAAQEQg1AAAgIhBqAABARCDUALDcsGHD1Lt3b2VmZtb72r59+wWfd8OGDbrlllskSX/605/Uo0cPs5oMIAzwyF0AITF79mzddtttpp5z9OjRGj16tKnnBBA+6KkB0OIMGzZML7/8sm666SZlZmZq4sSJ2rNnjyTJ7Xbr2Wef1bXXXqsBAwborrvu0pdffilJ+t3vfqdhw4Y1eM7du3froYce0jXXXKPrrrtOzz77rE6dOuV/35133qlf/OIXGjhwoAYNGqSnnnpKLpfLmgsGYApCDYAWae3atfrVr36lbdu2KS0tTY8++qhcLpfWr1+vnTt36sMPP9Qf//hH9e/fX7Nnzz7nuY4fP657771XV199tbZs2aJ3331X+/fv189+9jP/MTt27FCHDh20detWLVu2TB988IE+/vjjYF8mABMRagCExOzZs5WdnR3wNWrUKP/+Bx98UP/wD/+gVq1aaebMmfrb3/6mHTt2qFWrVvr222+1bt067d+/X1OnTtWGDRvO+bs2b96smJgY/fSnP1WrVq3UsWNHzZo1S//zP/+jo0ePSpJatWqlRx99VDExMerTp4969Oih/fv3B7UGAMzFnBoAIfHMM8+cc07NlVde6f85Pj5ebdu21dGjRzVy5Ei5XC698847+uUvf6kOHTro0Ucf1Z133tnouUpLS9WlSxfZ7Xb/tssvv1yS9N1330mSOnTooKioKP/+mJgYGYZxwdcHwHqEGgAt0pEjR/w/l5eX6/jx47rsssu0f/9+9erVS2PGjFFlZaU++ugjzZgxQ9nZ2Y2eq2vXriouLpbH4/EHm0OHDkmSOnbsqH379gX3YgBYguEnAC3SG2+8oYMHD6qiokLPPfecrrrqKmVmZuqTTz7RlClT9O2336pVq1Zq27atoqOjlZiY2Oi5hg4dKklatGiRKisrdfToUc2bN08DBw5U165drbokAEFGTw2AkHjmmWc0d+7cettzc3MlSVlZWXr88cdVXFys/v3769VXX5XNZtO9996rI0eO6I477tDp06fVtWtXvfDCC+rcuXOjvysxMVFvvPGGnn/+eX/AycnJCZgoDCD8RRkMGgNoYYYNG6YpU6aYfh8bAJGN4ScAABARCDUAACAiMPwEAAAiAj01AAAgIhBqAABARCDUAACAiECoAQAAEYFQAwAAIgKhBgAARARCDQAAiAiEGgAAEBEINQAAICL8f8+XmAq8B74yAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlabel('Epsilon')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(epsilons, accs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows that very small values of $\\epsilon$ result in far less accurate models. \n",
    "\n",
    "Keep in mind that the $\\epsilon$ we specify in the plot is a *per-iteration* $\\epsilon$, so the privacy cost is much higher after composition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
